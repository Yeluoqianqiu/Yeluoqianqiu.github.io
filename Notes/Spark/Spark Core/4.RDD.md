## 一、RDD简介

`RDD` 全称为 Resilient Distributed Datasets，是 Spark 最基本的数据抽象，它是**只读的、可分区的数据集合** ，支持**并行操作** ，可以由外部数据集或其他 RDD 转换而来，它具有以下特性：

* 一个 RDD 由一个或者多个分区（Partitions）组成。对于 RDD 来说，每个分区对应一个task，**用户可以在创建 RDD 时指定其分区个数，如果没有指定，则默认分区数为程序所分配到的 CPU 核心数** ；
* RDD 拥有一个用于计算分区的函数 compute()；
* RDD 会保存彼此间的依赖关系，RDD 的每次转换都会生成一个新的依赖关系，这种 RDD 之间的依赖关系就像流水线一样。**在部分分区数据丢失后，可以通过这种依赖关系重新计算丢失的分区数据** ，而不是对 RDD 的所有分区进行重新计算；
* Key-Value 型的 RDD 还拥有 Partitioner(分区器)，用于决定数据被存储在哪个分区中，目前 Spark 中支持 HashPartitioner(哈希分区器) 和 RangePatitioner(范围分区器)；

> 只有k-v型的RDD才需要分区，而HDFS上的数据默认已经分好区了，再者非k-v型的RDD分区意义不大，一定要分区也是可以的，通过创建时指定分区数或者通过partitionBy()设定分区。
>

* 一个优先位置列表 (可选)，用于存储每个分区的优先位置 (prefered locations)。对于一个 HDFS 文件来说，这个列表保存的就是每个分区所在的块的位置，**按照“移动数据不如移动计算“的理念，Spark 在进行任务调度的时候，会尽可能将计算任务分配到其所要处理数据块的存储位置。**

`RDD[T]` 抽象类的部分相关代码如下：

```
// 由子类实现以计算给定分区
def compute(split: Partition, context: TaskContext): Iterator[T]

// 获取所有分区
protected def getPartitions: Array[Partition]

// 获取所有依赖关系
protected def getDependencies: Seq[Dependency[_]] = deps

// 获取优先位置列表
protected def getPreferredLocations(split: Partition): Seq[String] = Nil

// 分区器 由子类重写以指定它们的分区方式
@transient val partitioner: Option[Partitioner] = None
```

## 二、创建RDD

RDD 有两种创建方式：

### 2.1 由现有集合创建

这里使用 `spark-shell` 进行测试，启动命令如下：

```
spark-shell --master local[4]
```

启动 `spark-shell` 后，程序会自动创建应用上下文，相当于执行了下面的 Scala 语句：

```
val conf = new SparkConf().setAppName("Spark shell").setMaster("local[4]")
val sc = new SparkContext(conf)
```

由现有集合创建 RDD，你可以在创建时指定其分区个数，**如果没有指定，则采用程序所分配到的 CPU 的核心数** ：

```
val data = Array(1, 2, 3, 4, 5)
// 由现有集合创建 RDD,默认分区数为程序所分配到的 CPU 的核心数
val dataRDD = sc.parallelize(data) 
// 查看分区数
dataRDD.getNumPartitions
// 明确指定分区数
val dataRDD = sc.parallelize(data,2)
```

> 还可以用`sc.makeRDD()`从集合创建RDD，其底层由`sc.parallelize()`实现。
>

执行结果如下：

![image.png](assets/image-20210409193238-ugg0bfp.png)

> makeRDD 和parallelize()可以传入一个`<strong>numSlices</strong>`参数来自定义分区数，若不指定，则分区数为`defaultParallelism = math.max(totalCoreCount.get(), 2))`，即可用的核数和2中的最大值。
>

### 2.2 从文件创建

引用外部存储系统中的数据集，例如本地文件系统，HDFS，HBase 或支持 Hadoop InputFormat 的任何数据源。

```
val fileRDD = sc.textFile("/usr/file/emp.txt")
fileRDD.take(1) // 获取第一行文本
```

使用外部存储系统时需要注意以下三点：

* 如果是集群模式，从本地文件系统读取数据，则要求集群中所有节点的相同路径上都存在该文件；
* 支持目录路径，支持压缩文件，支持使用通配符。
* `sc.textFile()`默认分区数为HDFS中的block(128M)的块数。也可以传入一个minPartitions参数，设定最小的分区数，若不指定，则取`defaultMinPartitions=math.min(defaultParallelism, 2)`，即**minPartitions默认为2** ，【minPartitions只是一个允许的最小值，具体多少分区取决于hadoop读取文件时的分片规则，具体说来，若minPartition指定为1或0，则原来几个文件，就有几个分区；若minPartition>=2，则该目录下的所有文件的字节数除以minPartition得到goalSize，取goalSize和HDFS block(128M)的较小值作为分区大小，一个文件总字节数小于分区大小的1.1倍，算作一个分区，否则，将该文件根据分区大小分为多个分区，所有文件的分区加起来就是这个任务输出的分区数了，但存储的时候按行存储，有的分区可能为空。详见https://www.jianshu.com/p/e33671341f0d】(???上集群试验一下)

### 2.3 textFile和wholeTextFiles

两者都可以用来读取外部文件，但是返回格式是不同的：

* **textFile** ：其返回格式是`RDD[String]` ，返回的是就是文件内容，RDD 中每一个元素对应一行数据；
* **wholeTextFiles** ：其返回格式是`RDD[(String, String)]`，元组中第一个参数是文件路径，第二个参数是文件内容；

两者默认分区数为HDFS块数，都可传入第二个参数来控制最小分区数；

```
def textFile(path: String,minPartitions: Int = defaultMinPartitions): RDD[String] = withScope {...}
def wholeTextFiles(path: String,minPartitions: Int = defaultMinPartitions): RDD[(String, String)]={..}
```

## 三、操作RDD

RDD 支持两种类型的操作：*transformations* （转换，从现有数据集创建新数据集）和 *actions* （在数据集上运行计算后将值返回到驱动程序）。RDD 中的所有转换操作都是惰性的，它们只是记住这些转换操作，但不会立即执行，**只有遇到 *action* 操作后才会真正的进行计算** ，这类似于函数式编程中的惰性求值。

```
val list = List(1, 2, 3)
// map 是一个 transformations 操作，而 foreach 是一个 actions 操作
sc.parallelize(list).map(_ * 10).foreach(println)
// 输出： 10 20 30
```

## 四、缓存RDD

### 4.1 缓存级别

Spark 速度非常快的一个原因是 RDD 支持缓存。成功缓存后，如果之后的操作使用到了该数据集，则直接从缓存中获取。虽然缓存也有丢失的风险，但是由于 RDD 之间的依赖关系，如果某个分区的缓存数据丢失，只需要重新计算该分区即可。

Spark 支持多种缓存级别 ：

| Storage Level（存储级别）                 | Meaning（含义）                                                                                                                                                     |
| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `MEMORY_ONLY`                             | 默认的缓存级别，将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中。如果内存空间不够，则部分分区数据将不再缓存。                                                      |
| -                                         | -                                                                                                                                                                   |
| `MEMORY_AND_DISK`                         | 将 RDD 以反序列化的 Java 对象的形式存储 JVM 中。如果内存空间不够，将未缓存的分区数据存储到磁盘，在需要使用这些分区时从磁盘读取。                                    |
| `MEMORY_ONLY_SER`                         | 将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式比反序列化对象节省存储空间，但在读取时会增加 CPU 的计算负担。仅支持 Java 和 Scala 。 |
| `MEMORY_AND_DISK_SER`                     | 类似于 `MEMORY_ONLY_SER`，但是溢出的分区数据会存储到磁盘，而不是在用到它们时重新计算。仅支持 Java 和 Scala。                                                        |
| `DISK_ONLY`                               | 只在磁盘上缓存 RDD                                                                                                                                                  |
| `MEMORY_ONLY_2`, `MEMORY_AND_DISK_2`, etc | 与上面的对应级别功能相同，但是会为每个分区在集群中的两个节点上建立副本。                                                                                            |
| `OFF_HEAP`                                | 与 `MEMORY_ONLY_SER` 类似，但将数据存储在堆外内存中。这需要启用堆外内存。                                                                                           |

> 启动堆外内存需要配置两个参数：
>
> * **spark.memory.offHeap.enabled** ：是否开启堆外内存，默认值为 false，需要设置为 true；
> * **spark.memory.offHeap.size** : 堆外内存空间的大小，默认值为 0，需要设置为正值。
>

![image.png](assets/image-20210409193259-b1erl56.png)

### 4.2 使用缓存

缓存数据的方法有两个：`persist()` 和 `cache()` 。`cache()` 内部调用的也是 `persist()`，它是 `persist()` 的特殊化形式，等价于 `persist(StorageLevel.MEMORY_ONLY)`。

注意：并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。

案例实操：

```
#（1）创建一个RDD
scala> val rdd = sc.makeRDD(Array("atguigu"))
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[19] at makeRDD at <console>:25
#（2）将RDD转换为携带当前时间戳不做缓存
scala> val nocache = rdd.map(_.toString+System.currentTimeMillis)
nocache: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[20] at map at <console>:27
#（3）多次打印结果
scala> nocache.collect
res0: Array[String] = Array(atguigu1538978275359)

scala> nocache.collect
res1: Array[String] = Array(atguigu1538978282416)

scala> nocache.collect
res2: Array[String] = Array(atguigu1538978283199)
#（4）将RDD转换为携带当前时间戳并做缓存
scala> val cache =  rdd.map(_.toString+System.currentTimeMillis).cache
cache: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[21] at map at <console>:27
#（5）多次打印做了缓存的结果
scala> cache.collect
res3: Array[String] = Array(atguigu1538978435705)                               

scala> cache.collect
res4: Array[String] = Array(atguigu1538978435705)

scala> cache.collect
res5: Array[String] = Array(atguigu1538978435705)
```

### 4.3 移除缓存

Spark 会自动监视每个节点上的缓存使用情况，并按照最近最少使用（LRU）的规则删除旧数据分区。当然，你也可以使用 `RDD.unpersist()` 方法进行手动删除。

## 五、检查点

Spark中对于数据的保存除了持久化操作之外，还提供了一种检查点的机制，检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。检查点通过将数据写入到HDFS文件系统实现了RDD的检查点功能。

为当前RDD设置检查点。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用sc.setCheckpointDir(hdfs://…)设置的。**在** **checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移除。** 对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。

案例实操：

```
#（1）设置检查点
scala> sc.setCheckpointDir("hdfs://hadoop102:9000/checkpoint")
#（2）创建一个RDD
scala> val rdd = sc.parallelize(Array("atguigu"))
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[14] at parallelize at <console>:24
#（3）将RDD转换为携带当前时间戳并做checkpoint
scala> val ch = rdd.map(_+System.currentTimeMillis)
ch: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[16] at map at <console>:26

scala> ch.checkpoint
#（4）多次打印结果
scala> ch.collect
res55: Array[String] = Array(atguigu1538981860336)

scala> ch.collect
res56: Array[String] = Array(atguigu1538981860504)

scala> ch.collect
res57: Array[String] = Array(atguigu1538981860504)

scala> ch.collect
res58: Array[String] = Array(atguigu1538981860504)
```

## 六、理解shuffle

### 5.1 shuffle介绍

在 Spark 中，一个任务对应一个分区，通常不会跨分区操作数据。但如果遇到 `reduceByKey` 等操作，Spark 必须从所有分区读取数据，并查找所有键的所有值，然后汇总在一起以计算每个键的最终结果 ，这称为 `Shuffle`。

![image.png](assets/image-20210409193310-mi3cy7r.png)

### 5.2 Shuffle的影响

Shuffle 是一项开销高昂的操作，因为它通常会跨节点操作数据，这会涉及磁盘 I/O，网络 I/O，和数据序列化。某些 Shuffle 操作还会消耗大量的堆内存，因为它们使用堆内存来临时存储需要网络传输的数据。Shuffle 还会在磁盘上生成大量中间文件，从 Spark 1.3 开始，这些文件将被保留，直到相应的 RDD 不再使用并进行垃圾回收，这样做是为了避免在计算时重复创建 Shuffle 文件。如果应用程序长期保留对这些 RDD 的引用，则垃圾回收可能在很长一段时间后才会发生，这意味着长时间运行的 Spark 作业可能会占用大量磁盘空间，通常可以使用 `spark.local.dir` 参数来指定这些临时文件的存储目录。

### 5.3 导致Shuffle的操作

由于 Shuffle 操作对性能的影响比较大，所以需要特别注意使用，以下操作都会导致 Shuffle：

* **涉及到重新分区操作** ： 如`repartition` 和`coalesce(n,true)`；【coalesce默认不shuffle，而repartition正是coalesce(n, shuffle=true)】
* **所有涉及到 ByKey 的操作** ：如`groupByKey` 和`reduceByKey`，【但`countByKey` 是行动算子，将所有分区汇总，因此不算shuffle】；
* **联结操作** ：如`cogroup` 和某些`join`。

## 七、血统、宽依赖和窄依赖

RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。

查看血统：

```
#（1）读取一个HDFS文件并将其中内容映射成一个个元组
scala> val wordAndOne = sc.textFile("/fruit.tsv").flatMap(_.split("\t")).map((_,1))
wordAndOne: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[22] at map at <console>:24
#（2）统计每一种key对应的个数
scala> val wordAndCount = wordAndOne.reduceByKey(_+_)
wordAndCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[23] at reduceByKey at <console>:26
#（3）查看“wordAndOne”的Lineage
scala> wordAndOne.toDebugString
res5: String =
(2) MapPartitionsRDD[22] at map at <console>:24 []
 |  MapPartitionsRDD[21] at flatMap at <console>:24 []
 |  /fruit.tsv MapPartitionsRDD[20] at textFile at <console>:24 []
 |  /fruit.tsv HadoopRDD[19] at textFile at <console>:24 []
#（4）查看“wordAndCount”的Lineage
scala> wordAndCount.toDebugString
res6: String =
(2) ShuffledRDD[23] at reduceByKey at <console>:26 []
 +-(2) MapPartitionsRDD[22] at map at <console>:24 []
    |  MapPartitionsRDD[21] at flatMap at <console>:24 []
    |  /fruit.tsv MapPartitionsRDD[20] at textFile at <console>:24 []
    |  /fruit.tsv HadoopRDD[19] at textFile at <console>:24 []
#（5）查看“wordAndOne”的依赖类型
scala> wordAndOne.dependencies
res7: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@5d5db92b)
#（6）查看“wordAndCount”的依赖类型
scala> wordAndCount.dependencies
res8: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@63f3e6a8)
```

RDD 和它的父 RDD(s) 之间的依赖关系分为两种不同的类型：

* **窄依赖 (narrow dependency)** ：父 RDDs 的一个分区最多被子 RDDs 一个分区所依赖；【独生】
* **宽依赖 (wide dependency)** ：父 RDDs 的一个分区可以被子 RDDs 的多个子分区所依赖。【超生】

> shuffle导致了宽依赖。
>

如下图，每一个方框表示一个 RDD，带有颜色的矩形表示分区：

![image.png](assets/image-20210409193319-0zijf1j.png)

区分这两种依赖是非常有用的：

* 首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）对父分区数据进行计算，例如先执行 map 操作，然后执行 filter 操作。而宽依赖则需要计算好所有父分区的数据，然后再在节点之间进行 Shuffle，这与 MapReduce 类似。
* 窄依赖能够更有效地进行数据恢复，因为只需重新对丢失分区的父分区进行计算，且不同节点之间可以并行计算；而对于宽依赖而言，如果数据丢失，则需要对所有父分区数据进行计算并再次 Shuffle。

## 八、DAG的生成

RDD(s) 及其之间的依赖关系组成了 DAG(Directed Acyclic Graph，有向无环图)，DAG 定义了这些 RDD(s) 之间的 Lineage(血统) 关系，通过血统关系，如果一个 RDD 的部分或者全部计算结果丢失了，也可以重新进行计算。Spark根据依赖关系的不同将 DAG 划分为不同的计算阶段 (Stage)：

* 对于窄依赖，由于分区的依赖关系是确定的，其转换操作可以在同一个线程执行，所以可以划分到同一个执行阶段；
* 对于宽依赖，由于 Shuffle 的存在，只能在父 RDD(s) 被 Shuffle 处理完成后，才能开始接下来的计算，**因此遇到宽依赖就需要重新划分阶段。**

![image.png](assets/image-20210409193325-qbtkxrb.png)