```sql
--查看系统自带函数
show functions;
--查看某个函数的用法
desc function [extended] upper;  --extended显示详细用法
```

## 1.常用内置函数

### 1.1 nvl

**1** **）函数说明**

给值为NULL的数据赋值，它的格式是`NVL( value, default_value)`。它的功能是如果value为NULL，则返回default_value，否则返回value。

**2** **）数据准备：采用员工表**

**3** **）查询：如果员工的** **comm** **为** **NULL** **，则用** **-1代替**

```
hive (default)> select comm,nvl(comm, -1) from emp;
OK
comm    _c1
NULL    -1.0
300.0   300.0
500.0   500.0
NULL    -1.0
1400.0  1400.0
NULL    -1.0
NULL    -1.0
NULL    -1.0
NULL    -1.0
0.0     0.0
NULL    -1.0
NULL    -1.0
NULL    -1.0
NULL    -1.0
```

**4** **）查询：如果员工的** **comm** **为** **NULL** **，则用领导** **id代替**

```
hive (default)> select comm, nvl(comm,mgr) from emp;
OK
comm    _c1
NULL    7902.0
300.0   300.0
500.0   500.0
NULL    7839.0
1400.0  1400.0
NULL    7839.0
NULL    7839.0
NULL    7566.0
NULL    NULL
0.0     0.0
NULL    7788.0
NULL    7698.0
NULL    7566.0
NULL    7782.0
```

### 1.2 CASE WHEN THEN ELSE END

**1** **）数据准备**

**创建** /opt/module/data/emp.txt

```
[atguigu@hadoop102 datas]$ vi emp.txt
悟空  A   男
大海  A   男
宋宋  B   男
凤姐  A   女
婷姐  B   女
婷婷  B   女
```

**2）** **创建** **hive** **表并导入数据**

```
create table emp(
name string,
dept_id string,
sex string)
row format delimited fields terminated by "\t";
load data local inpath '/opt/module/data/emp_sex.txt' into table emp;
```

**3）查看数据**

```
hive (default)> select * from emp;
OK
emp.name    emp.dept_id emp.sex
悟空    A       男
大海    A       男
宋宋    B       男
凤姐    A       女
婷姐    B       女
婷婷    B       女
```

**4）需求：** 求出不同部门男女各多少人。

**5）按需求查询数据**

```
select
  dept_id,
  sum(case when sex='男' then 1 else 0 end) m,
  sum(case when sex='女' then 1 else 0 end) f
from emp
group by dept_id;

dept_id m       f
A       2       1
B       1       2
```

### 1.3 if(expression, val1,val2)

```
select dept_id, sum(if(sex='男',1,0) ) as M, sum(if(sex='女',1,0) ) as F from emp group by dept_id;

dept_id m       f
A       2       1
B       1       2
```

### 1.4 CONCAT CONCAT_WS

* CONCAT(str1/col, str2/col…)：将任意个数的输入拼接为字符串，输入可以是string或数字，若有null，返回null 【mysql的concat只能接收两个参数】
* CONCAT_WS(separator, str1, str2,...)或concat_ws(separator, 集合)：它是一个特殊形式的 CONCAT()。第一个参数是分隔符，用于将后续参数拼接起来。**只接受string或**** array<string>** **的输入** ，如果分隔符是 NULL，返回NULL。这个函数会跳过分隔符后的任何 NULL 和空字符串。

```
select concat('*','1','2',3); --> *123
select concat('*','1','2',3,null); --> NULL

select concat_ws(null,'1','2','4');  --> NULL
select concat_ws('*','1','2',null,'4'); --> 1*2*4
```

### 1.5 行转列

* COLLECT_SET(col)：函数只接受非集合类型，结合group by，将同组数据的指定列收集并去重，得到set。
* collect_list(col)：是collect_set(col)的不去重版本。

**实例：** 有以下数据，把星座和血型一样的人归类到一起。

| name   | constellation | blood_type |
| ------ | ------------- | ---------- |
| 孙悟空 | 白羊座        | A          |
| 大海   | 射手座        | A          |
| 宋宋   | 白羊座        | B          |
| 猪八戒 | 白羊座        | A          |
| 凤姐   | 射手座        | A          |
| 苍老师 | 白羊座        | B          |

结果如下：

```
射手座,A            大海|凤姐
白羊座,A            孙悟空|猪八戒
白羊座,B            宋宋|苍老师
```

**4** **）创建本地** **constellation.txt** **，导入数据**

```
[atguigu@hadoop102 datas]$ vi constellation.txt
孙悟空  白羊座  A
大海    射手座  A
宋宋    白羊座  B
猪八戒  白羊座  A
唐僧    射手座  A
沙僧    白羊座  B
```

**5** **）创建** **hive** **表并导入数据**

```
create table person_info(
name string,
constellation string,
blood_type string)
row format delimited fields terminated by "\t";
load data local inpath "/opt/module/data/constellation.txt" into table person_info;
```

**6）按需求查询数据**

```
select concat(constellation,',', blood_type), concat_ws('|',collect_set(name)) from person_info group by constellation, blood_type;

白羊座,A        孙悟空|猪八戒
白羊座,B        宋宋|沙僧
射手座,A        大海|唐僧
```

### 1.5 列转行

* EXPLODE(col)：将hive一行中某个复杂的array或者map列拆分成多行。
* LATERAL VIEW：结合split, explode等UDTF使用，将udtf(expression)的结果放到一个虚拟表中，并将虚拟表的每一行和原输入行关联上，可以满足同时select原表字段和UDTF产生的字段的需求。语法：
* lateralView: LATERAL VIEW udtf(expression) tableAlias AS columnAlias (',' columnAlias)*
* fromClause: FROM baseTable (lateralView)*
* 即`select col1,col2 from t1 lateral view udtf(expression) t2 as col2;`

```
hive (default)> select explode(array(1,2,3)) as num;
hive (default)> select explode(map("name","xiaoming","age",23)) as (k,v);

create table pageAds(pageid string, adid_list array<int>);
insert into pageAds values ("front_page", array(1,2)), ("contact_page",array(3,4));
SELECT pageid, adid FROM pageAds LATERAL VIEW explode(adid_list) adTable AS adid;

pageid  adid
front_page      1
front_page      2
contact_page    3
contact_page    4
```

> 参考[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView)
>

### 1.6 窗口函数（开窗函数）

窗口函数的执行顺序**仅早于distinct和order by** ，用于对**前面的执行结果** 进行**开窗分析** 。

OVER()：**开窗** ，指定分析窗口大小，这个窗口大小可能会随着行变化而变化

over()有两种使用场景：

* **聚合函数之后** ，此场景支持开窗子句
* **序列函数之后** ，此场景不支持开窗子句

over()中可以接如下子句：

* `partition by`：作用类似distribute by，用在over()中，对数据按字段分组，即窗口。
* `order by`：对窗口内数据按字段排序。order by 子句在**row_number(), rank(), dense_rank(), lead(), lag()等序列函数场景中不可或缺** 。
* 开窗子句：`ROWS BETWEEN ... AND ...`，必须先使用order by排序，在分区内进一步缩小窗口大小，只用于聚合函数场景
* `CURRENT ROW`：当前行
* `n PRECEDING`：往前n行数据
* `n FOLLOWING`：往后n行数据
* `UNBOUNDED PRECEDING`：起点
* `UNBOUNDED FOLLOWING`：终点

示例：

* `ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`（从起点到当前行）
* `ROWS BETWEEN 2 PRECEDING AND 1 FOLLOWING`（往前2行到往后1行）
* `ROWS BETWEEN 2 PRECEDING AND CURRENT ROW`（往前2行到当前行）
* `ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING`（当前行到终点）

over()中不接子句表示对所有数据聚合。

**2）序列函数：不能使用窗口子句，必须有order by**

* `ntile(n)`：用于将分组数据按照顺序切分成n片，每行返回该行的切片值，编号从1开始。如果切片不均匀，前面的切片会多一行。注意：n必须为int类型。
* 排序相关：
* `rank()`：返回排序值，排名可以相等，会在名次中留下空位，如1,1,3,4
* `dense_rank()`：返回排序值，排名可以相等，不会在名次中留下空位，如1,1,2,3
* `row_number()`：返回排序值，排名不会相等，排序值相同时则根据表中记录的顺序排序，如1,2,3,4
* `percent_rank()`：

注意：使用排序函数时，NULL是最大的，降序排列时会排在最前面，影响排序结果。可以这样： `rank() over(order by score desc nulls last)`

* lag(col,n,default_val)：返回指定列从当前行后退n行的值，没有则返回default_value，没有默认值则返回NULL
* lead(col,n,default_val)：返回指定列从当前行前进n行的值，没有则返回default_value，没有默认值则返回NULL
* first_value：返回分组内排序后的第一个值
* last_value：返回分组内排序后，截止到当前行的最后一个值

**3）示例：**

数据准备：

order表，字段分别为name,orderdate,cost。数据如下：

```
jack,2015-01-01,10
tony,2015-01-02,15
jack,2015-02-03,23
tony,2015-01-04,29
jack,2015-01-05,46
jack,2015-04-06,42
tony,2015-01-07,50
jack,2015-01-08,55
mart,2015-04-08,62
mart,2015-04-09,68
neil,2015-05-10,12
mart,2015-04-11,75
neil,2015-06-12,80
mart,2015-04-13,94
```

在hive中建立一张order表，将数据插入进去。

```
create table orders(
name string,
orderdate string,
cost int
) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
load data local inpath "/opt/module/data/orders.txt" into table orders;
```

查询一：查询在2015年4月份购买过的顾客及总人数

```
select name,count(*) over ()
from orders
where substring(orderdate,1,7) = '2015-04'
group by name;

name count_window_0 
mart 2 
jack 2
```

查询二：求顾客的购买明细及月购买总额

```
select *, sum(cost) over(partition by name, substr(orderdate,1,7)) from orders;

orders.name     orders.orderdate        orders.cost     sum_window_0
mart    2015-04-08      62      299
mart    2015-04-09      68      299
mart    2015-04-11      75      299
mart    2015-04-13      94      299
neil    2015-05-10      12      12
neil    2015-06-12      80      80
jack    2015-01-01      10      111
jack    2015-01-05      46      111
jack    2015-01-08      55      111
jack    2015-02-03      23      23
jack    2015-04-06      42      42
tony    2015-01-02      15      94
tony    2015-01-04      29      94
tony    2015-01-07      50      94
```

> partition by 可以替换为distribute by
>

查询三：按照name进行分区，按照购物时间进行排序，做cost的累加

```
select name,orderdate,cost,
sum(cost) over() as sample1,--所有行相加
sum(cost) over(partition by name) as sample2,--按name分组，组内数据相加
sum(cost) over(partition by name order by orderdate) as sample3,--按name分组，组内数据累加
sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row )  as sample4 ,--和sample3一样,由起点到当前行的聚合
sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING   and current row) as sample5, --当前行和前面一行做聚合
sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING   AND 1 FOLLOWING  ) as sample6,--当前行和前边一行及后面一行
sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行
from orders;

name    orderdate   cost    sample1 sample2 sample3 sample4 sample5 sample6 sample7
jack    2015-01-01  10  661 176 10  10  10  56  176
jack    2015-01-05  46  661 176 56  56  56  111 166
jack    2015-01-08  55  661 176 111 111 101 124 120
jack    2015-02-03  23  661 176 134 134 78  120 65
jack    2015-04-06  42  661 176 176 176 65  65  42
mart    2015-04-08  62  661 299 62  62  62  130 299
mart    2015-04-09  68  661 299 130 130 130 205 237
mart    2015-04-11  75  661 299 205 205 143 237 169
mart    2015-04-13  94  661 299 299 299 169 169 94
neil    2015-05-10  12  661 92  12  12  12  92  92
neil    2015-06-12  80  661 92  92  92  92  92  80
tony    2015-01-02  15  661 94  15  15  15  44  94
tony    2015-01-04  29  661 94  44  44  44  94  79
tony    2015-01-07  50  661 94  94  94  79  79  50
```

序列函数的应用举例：

查询一：ntile()

假如我们想要每位顾客购买金额前1/3的交易记录,我们便可以使用ntile()

```
select name,orderdate,cost,
ntile(3) over() as sample1 , --全局数据切片
ntile(3) over(partition by name), -- 按照name进行分组,在分组内将数据切成3份
ntile(3) over(order by cost),--全局按照cost升序排列,数据切成3份
ntile(3) over(partition by name order by cost ) --按照name分组，在分组内按照cost升序排列,数据切成3份
from orders;

name    orderdate   cost    sample1 sample2 sample3 sample4
jack    2015-01-01  10  3   1   1   1
jack    2015-02-03  23  3   1   1   1
jack    2015-04-06  42  2   2   2   2
jack    2015-01-05  46  2   2   2   2
jack    2015-01-08  55  2   3   2   3
mart    2015-04-08  62  2   1   2   1
mart    2015-04-09  68  1   2   3   1
mart    2015-04-11  75  1   3   3   2
mart    2015-04-13  94  1   1   3   3
neil    2015-05-10  12  1   2   1   1
neil    2015-06-12  80  1   1   3   2
tony    2015-01-02  15  3   2   1   1
tony    2015-01-04  29  3   3   1   2
tony    2015-01-07  50  2   1   2   3
```

取sample4 = 1的那部分数据就是所需结果。

查询二：排序函数

```
select name, orderdate, cost,
rank() over(partition by name order by int(cost/10)) as rk,
dense_rank() over(partition by name order by int(cost/10)) as dr,
row_number() over(partition by name order by int(cost/10)) as rn
from orders;

name    orderdate       cost    rk      dr      rn
jack    2017-01-01      10      1       1       1
jack    2017-02-03      23      2       2       2
jack    2017-01-05      46      3       3       3
jack    2017-04-06      42      3       3       4
jack    2017-01-08      55      5       4       5
mart    2017-04-08      62      1       1       1
mart    2017-04-09      68      1       1       2
mart    2017-04-11      75      3       2       3
mart    2017-04-13      94      4       3       4
neil    2017-05-10      12      1       1       1
neil    2017-06-12      80      2       2       2
tony    2017-01-02      15      1       1       1
tony    2017-01-04      29      2       2       2
tony    2017-01-07      50      3       3       3
```

> int(cost/10))是强行造数据，懂意思就行。
>

查询三：lag()实现查看顾客上次的购买时间

```
select name,orderdate,cost,
lag(orderdate,1,'1900-01-01') over(partition by name order by orderdate) as time1,
lag(orderdate,2) over(partition by name order by orderdate) as time2
from orders;

name    orderdate   cost    time1   time2
jack    2015-01-01  10  1900-01-01  NULL
jack    2015-01-05  46  2015-01-01  NULL
jack    2015-01-08  55  2015-01-05  2015-01-01
jack    2015-02-03  23  2015-01-08  2015-01-05
jack    2015-04-06  42  2015-02-03  2015-01-08
mart    2015-04-08  62  1900-01-01  NULL
mart    2015-04-09  68  2015-04-08  NULL
mart    2015-04-11  75  2015-04-09  2015-04-08
mart    2015-04-13  94  2015-04-11  2015-04-09
neil    2015-05-10  12  1900-01-01  NULL
neil    2015-06-12  80  2015-05-10  NULL
tony    2015-01-02  15  1900-01-01  NULL
tony    2015-01-04  29  2015-01-02  NULL
tony    2015-01-07  50  2015-01-04  2015-01-02
```

查询四：first_value()和last_value()

```
select name,orderdate,cost,
first_value(orderdate) over(partition by name order by orderdate) as time1,
last_value(orderdate) over(partition by name order by orderdate) as time2
from orders;

name    orderdate   cost    time1   time2
jack    2015-01-01  10  2015-01-01  2015-01-01
jack    2015-01-05  46  2015-01-01  2015-01-05
jack    2015-01-08  55  2015-01-01  2015-01-08
jack    2015-02-03  23  2015-01-01  2015-02-03
jack    2015-04-06  42  2015-01-01  2015-04-06
mart    2015-04-08  62  2015-04-08  2015-04-08
mart    2015-04-09  68  2015-04-08  2015-04-09
mart    2015-04-11  75  2015-04-08  2015-04-11
mart    2015-04-13  94  2015-04-08  2015-04-13
neil    2015-05-10  12  2015-05-10  2015-05-10
neil    2015-06-12  80  2015-05-10  2015-06-12
tony    2015-01-02  15  2015-01-02  2015-01-02
tony    2015-01-04  29  2015-01-02  2015-01-04
tony    2015-01-07  50  2015-01-02  2015-01-07
```

### 1.7 日期相关函数

```
#1. current_date() 返回当前日期
hive> select current_date();
2021-03-13

#2. current_timestamp()返回当前日期和时间，等价于MySQL的now()
hive (default)> select current_timestamp();
2021-03-13 15:53:02.616

#3. date_add()/date_sub() 日期的加减
hive (default)> select date_add(current_date(),90);
2021-06-11
hive (default)> select date_sub("2021-03-01",1);
2021-02-28

#4. datediff() 求日期差
SELECT datediff(CURRENT_DATE(), "1990-06-04");
11240

#5. next_day(start_date, day_of_week)  返回今日后的下一个周几
hive (default)> select current_date(), next_day(current_date(),'sun');
2021-03-13      2021-03-14

#6. date_format()根据格式整理日期
hive (default)> select date_format(current_date(),'yyyyMMdd');
20210313
#7. last_day()当月最后一天的日期
hive (default)> select last_day(current_date());
2021-03-31
```

mysql除了current_date()，current_timestamp()外，还有now()

```
mysql> select current_date(),current_timestamp(),now();
+----------------+---------------------+---------------------+
| current_date() | current_timestamp() | now()               |
+----------------+---------------------+---------------------+
| 2021-03-13     | 2021-03-13 15:56:06 | 2021-03-13 15:56:06 |
+----------------+---------------------+---------------------+
```

### 1.8 get_json_object

设person表的xjson字段有数据：

`[{"name":"王二狗","age":"25"},{"name":"李狗嗨","age":"47"}]`

```
# 取json数组的某个元素
SELECT get_json_object(xjson,"$.[0]") FROM person;
{"name":"王二狗","age":"25"}
# 取json对象的某个属性值：
SELECT get_json_object(xjson,"$.[0].age") FROM person;
25
```

### 1.9 其他

* instr(str, substr)返回substr在string中的起始index，从1开始，失败返回0
* regexp_replace(str,'exp1','exp2')
* substr(str, pos[, len]): returns the substring of str that starts at pos and is of length len

## 2. 自定义函数

当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数。

根据用户自定义函数类别分为以下三种：

* UDF(User-Defined-Function)：一进一出
* UDAF(User-Defined Aggregation Function)：聚集函数，多进一出。如count()/max()/min()
* UDTF(User-Defined Table-Generating Functions)：一进多出，如explode()

> 官方文档地址：https://cwiki.apache.org/confluence/display/Hive/HivePlugins
>

编程步骤：

1）继承父类：

* `<span>org.apache.hadoop.hive.ql.udf.generic.GenericUDF</span>`
* `org.apache.hadoop.hive.ql.udf.generic.GenericUDTF`

2）需要实现evaluate函数；evaluate函数支持重载；

3）在hive的命令行窗口创建函数

添加jar

```
add jar jar_path
```

创建function

* temporary UDF可以跨database使用，但重启hive服务不可用
* 不带temporary的UDF，不能跨database使用，重启hive服务仍可用

```
create [temporary] function [dbname.]function_name AS class_name;
```

（4）在hive的命令行窗口删除函数

```
Drop [temporary] function [if exists] [dbname.]function_name;
```

注意：UDF必须要有返回类型，可以返回null，但是返回类型不能为void；

### 自定义UDF函数

**1）创建一个Maven工程Hive**

**2）导入依赖**

```
<dependencies>
        <dependency>
             <groupId>org.apache.hive</groupId>
             <artifactId>hive-exec</artifactId>
             <version>3.1.2</version>
        </dependency>
</dependencies>
```

**3** **）** **创建一个类**

继承UDF类，并重写`evaluate()`方法

```
package com.atguigu.hive;
import org.apache.hadoop.hive.ql.exec.UDF;
 
public class Lower extends UDF {
    public String evaluate (final String s) {
        if (s == null) {
             return null;
        }
        return s.toLowerCase();
    }
}
```

**4）** **打成jar包上传到服务器/opt/module/jars/udf.jar**

**5** **）** **将** **jar** **包添加到** **hive** **的** **classpath**

```
hive (default)> add jar /opt/module/data/udf.jar;
```

**6** **）创建临时函数与开发好的java class** **关联**

```
hive (default)> create temporary function mylower as "com.atguigu.hive.Lower";
```

7）即可在hql中使用自定义的函数

```
hive (default)> select ename, mylower(ename) lowername from emp;
```

### 自定义UDTF

0） 需求
自定义一个 UDTF 实现将一个任意分割符的字符串切割成独立的单词，例如：

```
hive(default)> select myudtf("hello,world,hadoop,hive", ",");
hello
world
hadoop
hive
```

1） 代码实现

```
package hive;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

import java.util.ArrayList;
import java.util.List;

public class UDTF extends GenericUDTF {
    private ArrayList<String> outList = new ArrayList<>();

    @Override
    public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException {
        //1.定义输出数据的列名和类型
        List<String> fieldNames = new ArrayList<>();
        List<ObjectInspector> fieldOIs = new ArrayList<>();
        //2.添加输出数据的列名和类型
        fieldNames.add("lineToWord");
        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
    }

    @Override
    public void process(Object[] args) throws HiveException {
        //1.获取原始数据
        String arg = args[0].toString();
        //2.获取数据传入的第二个参数，此处为分隔符
        String splitKey = args[1].toString();
        //3.将原始数据按照传入的分隔符进行切分
        String[] fields = arg.split(splitKey);
        //4.遍历切分后的结果，并写出
        for (String field : fields) {
            //集合为复用的，首先清空集合
            outList.clear();
            //将每一个单词添加至集合
            outList.add(field);
            //将集合内容写出
            forward(outList);
        }
    }

    @Override
    public void close() throws HiveException {
    }
}
```

2） 打成 jar 包上传到服务器/opt/module/hive/data/myudtf.jar
3） 将 jar 包添加到 hive 的 classpath 下

```
hive (default)> add jar /opt/module/hive/data/myudtf.jar;
```

4） 创建临时函数与开发好的 java class 关联

```
hive (default)> create temporary function myudtf as "com.atguigu.hive.MyUDTF";
```

5） 使用自定义的函数

```
hive (default)> select myudtf("hello,world,hadoop,hive",",");
```