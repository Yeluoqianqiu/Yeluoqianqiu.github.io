{
    "root": {
        "data": {
            "note": "",
            "id": "root",
            "text": "大数据"
        },
        "children": [
            {
                "data": {
                    "id": "5550f45d9167",
                    "text": "数仓建模理论",
                    "expandState": "expand",
                    "layout": null
                },
                "children": [
                    {
                        "data": {
                            "note": "存储海量数据的仓库，三大功能：对数据进行存储、管理和分析。会对数据清洗、转换、分类、聚合、拆分，基于数仓模型进行**分层**，建立维度表和事实表，用于后续的**报表分析**、**用户画像**、**数据挖掘**和**决策支持**等场景。有以下几个特点：\n1.多数据源：从前端埋点日志、业务数系统、财务系统等数据源经过ETL汇总到数仓中\n2.历史快照：数仓保存了相当长一段时间内的历史数据，反应了数据的生命周期，可以得到某个时间点的历史快照，常常会有一些拉链表、增量更新表、全量更新表\n3.查询为主：数仓不适合传统数据库的频繁增删改场景，主要是基于历史数据进行统计、整合然后联机分析\n4.集群分布：数据分布到集群的各个节点上，每个节点只保存数仓中的部分数据",
                            "id": "4523804a93dd",
                            "text": "数仓的概念",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "note": "|          | 数据仓库                                                     | 关系型数据库                   |\n| -------- | ------------------------------------------------------------ | ------------------------ |\n| 数据状态 | 很长一段时间的全量数据，反应变化，可获取历史快照 | 当前的最新数据           |\n| 数据操作 | 查询和新增                                                   | 频繁的增删改查           |\n| 应用场景 | 面向分析决策,OLAP                                               | 面向业务流程,OLTP             |\n| 设计理念 | 不严格遵循范式，有星型模型和雪花模型，数据冗余高             | 严格遵循范式，数据冗余小 |\n| 数据量   | 大批量、高吞吐、有延迟                                       | 小批次、高并发、低延迟   |",
                            "id": "e445472534cd",
                            "text": "数据仓库和关系型数据库的区别",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "note": "OLTP：**面向事务的随机读写**，**满足3NF**，关注**事务一致性与极低的数据冗余**；\nOLAP：**基于大批量数据的决策分析**，不关注事务一致性，也不过分关注低冗余，更关注**数据整合**，以及在**复杂大查询**下的**处理性能**。",
                            "id": "74a3a71d477f",
                            "text": "OLAP和OLTP区别",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "note": "狭义上主要指应用层，为用户提供数据支撑。\n广义上：所有以主题划分、可供查阅的数仓层次都可以成为数据集市，包括DWD、DWS、ADS层。如：订单主题，可以提供使用者从明细、维度聚合、应用分析各个层次的数据。用户、商品、活动等其他主题类似。",
                            "id": "a6032259e593",
                            "text": "数据集市",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "id": "b8e365bfa7e7",
                            "text": "数仓建模",
                            "layout": null
                        },
                        "children": [
                            {
                                "data": {
                                    "note": "**分治**：将复杂的问题分而治之，结构清晰，层层递进，越上层数据对应用越友好\n**复用**：沉淀公共数据，避免烟囱式开发，极大减少不必要的数据冗余，降低数据存储和计算成本。\n**规范**：设定schema规范，统一数据口径，方便元数据管理。\n**隔离**：下层业务逻辑变化对上层零感知，出错时只需修复问题相关层\n\n---\n\n也可从经典的性能、成本、效率、质量四个方面回答：\n- 性能：快速响应，IO占用低\n- **成本**：数据冗余小，存储、计算成本低\n- **效率**：用户使用数据体验好，效率高\n- 质量：统一数据口径，减少数据计算出错\n\n一个好的数据模型是在四者之间的权衡，核心是成本和效率，性能好自然节省成本，质量好自然效率高。\n\n",
                                    "id": "079f46eedc30",
                                    "text": "为什么要分层建模",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "note": "",
                                            "id": "1a073848d43c",
                                            "text": "数仓具体分层",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            },
                            {
                                "data": {
                                    "note": "采用ER模型建设数据仓库模型的出发点是整合数据，将各个系统中的数据以整个企业的高度按主题进行相似性组合和合并，并进行一致性处理，为数据分析决策服务，但并不能直接用于分析决策。特点是：\n\n- 模型从全企业的高度进行设计，需要全面了解企业业务和数据\n- 实施周期非常长\n- 对建模人员的能力要求非常高\n\n其建模步骤分为三个阶段：\n\n- 高层模型：一个高度抽象的模型，描述主要的主题以及主题间的关系，用于描述企业的业务总体概况。\n- 中层模型：在高层模型的基础上，细化主题的数据项。\n- 物理模型(底层模型)：在中层模型的基础上，考虑物理存储，同时基于性能和平台特点进行物理属性的设计，也可能做一些表的合并、分区的设计等。",
                                    "id": "b8df1e159a84",
                                    "text": "范式建模(ER建模)",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "id": "ck7oplgoxb40",
                                            "created": 1653364247068,
                                            "text": "3NF",
                                            "note": "第三范式：\n\n- 属性列具有原子性，不可再分。是最低级别的要求。\n- 非主属性完全依赖于主键，而非主键的一部分。第二范式消除了部分依赖，可以大幅减少数据冗余。\n- 非主属性不能传递依赖于主键。第三范式消除了传递依赖，进一步减少了数据冗余。"
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "id": "ck7owltlus80",
                                            "created": 1653364796398,
                                            "text": "补充",
                                            "note": "遵循范式可以大幅减少数据冗余，数据表更新快，体积小；缺点是一次查询常常需要关联多张表，导致性能降低，更难进行索引优化。因此，没有冗余的数据库未必是最好的数据库，有时为了提高运行效率，就必须降低范式标准，适当保留冗余数据。具体做法是：在概念数据模型设计时遵守第三范式，在物理模型设计阶段降低范式标准，具体操作即增加冗余字段，达到以空间换时间的目的。\n\n〖例〗：订单表中已经存在“单价”和“数量”两个字段，再增加“金额”(金额=单价×数量)这个冗余字段，虽然破坏了3NF，但可以提高查询统计的速度。"
                                        },
                                        "children": []
                                    }
                                ]
                            },
                            {
                                "data": {
                                    "id": "7572e212d3ac",
                                    "text": "维度建模",
                                    "layout": null,
                                    "note": "维度建模从分析决策的需求出发构建模型，重点关注用户如何**快速完成需求分析**，以及**在大规模复杂查询下的响应性能**。其典型代表是**星形模型**，以及某些特殊场景下的**雪花模型**。"
                                },
                                "children": [
                                    {
                                        "data": {
                                            "note": "1. **选择业务过程**：根据分析目标是事件的**过程**，还是其某个**状态**，或是事件的**流转效率**，可以将业务过程分为：\n\t- 单个业务事件过程：如交易的支付、退款过程\n    - 某个事件的状态：如当前的账户余额\n    - 一系列相关业务时事件组成的业务流程\n2. **声明粒度**：在事件分析时预判所有分析需要细分的程度，从而决定选择的粒度。粒度是维度的一个组合。\n3. **确定维度**：基于选好的粒度设计维表，包括维度属性，用于分析时进行分组和筛选。\n4. **确定事实**：确定分析需要衡量的指标。",
                                            "id": "c85b7d0dc188",
                                            "text": "建模流程",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "原则一、维度建模的最底层使用详细的原子数据，以支持上层应用对数据从不同维度、粒度进行挖掘洞察的需求。\n原则二、围绕业务过程构建维度模型\n业务过程是组织执行的活动，它们代表可测量的事件，如下一个订单或做一次结算，业务过程通常会捕获或生成唯一的与某个事件相关的性能指标，这些数据转换成事实后，每个业务过程都用一个原子事实表表示，除了单个过程事实表外，有时会从多个过程事实表合并成一个事实表，合并事实表是对单一流程事实表的一个很好的补充，并不能代替它们。\n原则三、确保每个事实表都有一个与之关联的日期维度表\n原则二中描述的可测量事件总有一个日期戳信息，每个事实表至少都有一个外键，关联到一个日期维度表，它的粒度就是一天，使用日历属性和非标准的关于测量事件日期的特性，如财务月和公司假日指示符，有时一个事实表中有多个日期外键。\n原则四、确保每个事实表中的事实具有相同的粒度或同级的详细程度\n在组织事实表时粒度上有三个基本原则：事务，周期快照或累加快照。无论粒度类型如何，事实表中的度量单位都必须达到相同水平的详细程度，如果事实表中的事实表现的粒度不一样，企业用户会被搞晕，BI应用程序会很脆弱，或者返回的结果根本就不对。\n原则五、解决事实表中的多对多关系\n由于事实表存储的是业务流程事件的结果，因此在它们的外键之间存在多对多(M:M)的关系，如多个仓库中的多个产品在多天销售，这些外键字段不能为空，有时一个维度可以为 单个测量事件赋予多个值，如一个保健对应多个诊断，或多个客户有一个银行账号，在这些情况下，它的不合理直接解决了事实表中多值维度，这可能违反了测量事件的天然粒度，因此我们使用多对多，双键桥接表连接事实表。\n原则六、解决维度表中多对一的关系\n属性之间分层的、多对一(M：1)的关系通常未规范化，或者被收缩到扁平型维度表中，如果你曾经有过为事务型系统设计实体关系模型的经历，那你一定要抵抗住旧有的思维模式，要将其规范化或将M:1关系拆分成更小的子维度，维度反向规范化是维度建模中常用的词汇。\n在单个维度表中多对一(M:1)的关系非常常见，一对一的关系，如一个产品描述对应一个产品代码，也可以在维度表中处理，在事实表中偶尔也有多对一关系，如详细当维度表中有上百万条记录时，它推出的属性又经常发生变化。不管怎样，在事实表中要慎用M:1关系。\n原则七、存储报告标记和过滤维度表中的范围值\n更重要的是，编码和关联的解码及用于标记和查询过滤的描述符应该被捕获到维度表中，避免在事实表中存储神秘的编码字段或庞大的描述符字段，同样，不要只 在维度表中存储编码，假定用户不需要描述性的解码，或它们将在BI应用程序中得到解决。如果它是一个行/列标记或下拉菜单过滤器，那么它应该当作一个维度 属性处理。\n尽管我们在原则5中已经陈述过，事实表外键不应该为空，同时在维度表的属性字段中使用“NA”或另一个默认值替换空值来避免空值也是明智的，这样可以减少用户的困惑。\n原则八、确定维度表使用了代理键\n按顺序分配代理键(除了日期维度)可以获得一系列的操作优势，包括更小的事实表、索引以及性能改善，如果你正在跟踪维度属性的变化，为每个变化使用一个 新的维度记录，那么确实需要代理键，即使你的商业用户没有初始化跟踪属性改变的设想值，使用代理也会使下游策略变化更宽松，代理也允许你使用多个业务键映 射到一个普通的配置文件，有利于你缓冲意想不到的业务活动，如废弃产品编号的回收或收购另一家公司的编码方案。\n原则九、创建一致的维度集成整个企业的数据\n对于企业数据仓库一致的维度(也叫做通用维度、标准或参考维度)是最基本的原则，在ETL系统中管理一次，然后在所有事实表中都可以重用，一致的维度在 整个维度模型中可以获得一致的描述属性，可以支持从多个业务流程中整合数据，企业数据仓库总线矩阵是最关键的架构蓝图，它展现了组织的核心业务流程和关联 的维度，重用一致的维度可以缩短产品的上市时间，也消除了冗余设计和开发过程，但一致的维度需要在数据管理和治理方面有较大的投入。\n原则十、不断平衡需求和现实，提供用户可接受的并能够支持他们决策的DW/BI解决方案\n维度建模需要不断在用户需求和数据源事实之间进行平衡，才能够提交可执行性好的设计，更重要的是，要符合业务的需要，需求和事实之间的平衡是DW/BI 从业人员必须面对的事实，无论是你集中在维度建模，还是项目策略、技术/ETL/BI架构或开发/维护规划都要面对这一事实。",
                                            "id": "19307715cce8",
                                            "text": "维度建模十大原则",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "id": "9df0040ed7b0",
                                            "text": "维度表和事实表",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "宽表从字面意义上讲就是字段比较多的数据库表。\n\n通常是指业务主题相关的指标、维度、属性关联在一起的一张数据库表。\n\n不是说列数多就是宽表，二是要包含较多的维度和指标\n\nDWS层常常是一些汇总大宽表",
                                            "id": "b3a936b6bfda",
                                            "text": "大宽表",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "https://cloud.tencent.com/developer/article/1122464",
                                            "id": "e6d8cd76c71e",
                                            "text": "星型模型、雪花模型与星座模型",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            },
                            {
                                "data": {
                                    "id": "ck7y7ujnly80",
                                    "created": 1653391067383,
                                    "text": "阿里OneData方法体系",
                                    "note": "一套统一化的集团数据整合及管理方法体系，包括一致性的**指标定义体系**、**模型设计方法体系**以及**配套工具**。"
                                },
                                "children": [
                                    {
                                        "data": {
                                            "id": "ck7yfngpfdc0",
                                            "created": 1653391678880,
                                            "text": "方法论核心",
                                            "note": "从业务架构设计到模型设计，从数据研发到数据服务，做到数据可管理、可追溯、可规避重复建设。"
                                        },
                                        "children": []
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "data": {
                            "note": "",
                            "id": "1cbe8bbec8ff",
                            "text": "拉链表",
                            "layout": null
                        },
                        "children": []
                    }
                ]
            },
            {
                "data": {
                    "note": "",
                    "id": "0db8c8507f0b",
                    "text": "Hadoop",
                    "layout": null
                },
                "children": []
            },
            {
                "data": {
                    "id": "95a945a7bdcc",
                    "text": "Hive",
                    "expandState": "collapse",
                    "layout": null
                },
                "children": [
                    {
                        "data": {
                            "note": "|    | Hive         | RDBMS                |\n| -------- | ------------ | -------------------- |\n| 数据存储 | HDFS         | local FS             |\n| 数据规模 | 大           | 小                   |\n| 数据操作 | 追加         | 行级别的增删改       |\n| 执行延迟 | 高           | 低                   |\n| 应用场景 | 海量数据OLAP | 常常是少量数据的OLTP |",
                            "id": "e62e534fa10a",
                            "text": "Hive与RDBMS的主要区别",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "note": "",
                            "id": "511a08e9c8b5",
                            "text": "Hive 数据类型",
                            "layout": null
                        },
                        "children": [
                            {
                                "data": {
                                    "note": "| 大类       | 类型                                                                                                                                                                                                |\n| ------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| 整型       | 有符号整数 TINYINT: 1byte，SMALLINT: 2byte，INT: 4byte，BIGINT: 8byte<br />                                                                                                                             |\n| 布尔型     | BOOLEAN—TRUE/FALSE                                                                                                                                                                                 |\n| 浮点型     | FLOAT: 单精度浮点型，DOUBLE: 双精度浮点型<br />                                                                                                                                                         |\n| 定点型     | DECIMAL(7,2)                                                                                                                                                                                        |\n| 字符串     | STRING：指定字符集的字符序列<br />VARCHAR：具有最大长度限制的字符序列<br />CHAR—固定长度的字符序列<br />                                                                                                       |\n| 日期和时间 | TIMESTAMP：时间戳。提交和查询时不会根据当地时间进行转换。<br />TIMESTAMP WITH LOCAL TIME ZONE：时间戳。提交给数据库时会根据数据库所在时区进行转换，查询时则转换为客户端所在时区的时间。<br />DATE—日期类型 |\n| 二进制     | BINARY—字节序列                                                                                                                                                                                    |",
                                    "id": "fe947047bae0",
                                    "text": "基本类型",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "| 类型   | 描述                                                                                | 示例                                   |\n| ------ | ----------------------------------------------------------------------------------- | -------------------------------------- |\n| STRUCT | 类似于结构体，是字段的集合，字段的类型可以不同，可以使用 `名称.字段名` 方式进行访问 | STRUCT ('xiaoming', 12 , '2018-12-12') |\n| MAP    | 键值对的集合，可以使用 `名称[key]` 的方式访问对应的值                               | map('a', 1, 'b', 2)                    |\n| ARRAY  | 数组是一组具有相同类型和名称的变量的集合，可以使用 `名称[index]` 访问对应的值       | ARRAY('a', 'b', 'c', 'd')              |",
                                    "id": "abf04a03f05a",
                                    "text": "复杂类型",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "3911be501f1d",
                                    "text": "数据类型转换",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "note": "![](https://yeluoqianqiu.github.io/assets/20210711224125.png)\n\n隐式转换方向如箭头所示，特别注意：\n\n* string可以隐式转换为double\n* bigint可以隐式转换为float。\n* BOOLEAN类型不可以转换为任何其它的类型。",
                                            "id": "ee50fa07537d",
                                            "text": "隐式转换",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "显示转换即使用`cast`进行强制类型转换。\n\n例如`CAST(&#39;1&#39; AS INT)`将string转换成int；强制类型转换失败时，如执行`CAST(&#39;X&#39; AS INT)`，表达式返回`NULL`。\n\n```\nhive&gt; select &#39;1&#39;+2, cast(&#39;1&#39; as int) + 2;\n+------+------+--+\n| _c0  | _c1  |\n+------+------+--+\n| 3.0  | 3    |\n+------+------+--+\n```\n\n&gt; mysql会直接解析该字符串的值：\n&gt;\n&gt; ```sql\n&gt; mysql&gt; select &quot;1&quot; + 2, &quot;1.0&quot; + 2, &quot;1.2&quot; + 2;\n&gt; +---------+-----------+-----------+\n&gt; | &quot;1&quot; + 2 | &quot;1.0&quot; + 2 | &quot;1.2&quot; + 2 |\n&gt; +---------+-----------+-----------+\n&gt; |       3 |         3 |       3.2 |\n&gt; +---------+-----------+-----------+\n&gt; ```",
                                            "id": "aad215327e47",
                                            "text": "显示转换",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "data": {
                            "id": "c47ea49ed5f2",
                            "text": "常用命令",
                            "layout": null
                        },
                        "children": [
                            {
                                "data": {
                                    "note": "`hive -e \"select ...\"` 执行sql\n\n`hive -f my_query.sql` 执行sql文件\n\n`hive --hiveconf mapred.reduce.tasks=4;` 启动Hive时指定命令行参数\n\n",
                                    "id": "01dde758f7fb",
                                    "text": "命令行常用命令",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "```shell\n# 可以直接使用hdfs命令\nhive>hdfs dfs -ls;\n\n# 使用set查看和修改配置，当前回话内有效\nhive>set mapred.reduce.tasks;  --查看\nhive>set mapred.reduce.tasks=4;  --修改\n```",
                                    "id": "c14530354697",
                                    "text": "hive交互窗口命令",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "配置的优先顺序由低到高：\n`hive-site.xml -> hivemetastore-site.xml -> hiveserver2-site.xml -> --hiveconf -> set`",
                                    "id": "39c6acabc3b4",
                                    "text": "配置的优先级",
                                    "layout": null
                                },
                                "children": []
                            }
                        ]
                    },
                    {
                        "data": {
                            "id": "6a8f9344ce5c",
                            "text": "DDL",
                            "layout": null
                        },
                        "children": [
                            {
                                "data": {
                                    "note": "",
                                    "id": "ea71983e88e1",
                                    "text": "database",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "```sql\nCREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name     --表名\n  (col_name data_type [COMMENT col_comment],\n    ... )  --列名与数据类型\n  [COMMENT table_comment]   --表描述\n  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]  --分区表分区规则\n  [\n    CLUSTERED BY (col_name, col_name, ...) \n   [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS\n  ]  --分桶表分桶规则\n  [SKEWED BY (col_name, col_name, ...) ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)  \n   [STORED AS DIRECTORIES] \n  ]  --指定倾斜列和值\n  [\n   [ROW FORMAT DELIMITED row_format]  \n   [STORED AS file_format]\n     | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]  \n  ]  -- 指定行分隔符、存储文件格式或采用自定义存储格式\n  [LOCATION hdfs_path]  -- 指定表的存储位置\n  [TBLPROPERTIES (key=value, ...)]  --指定表的属性\n  [AS select_statement];   --从查询结果创建表\n```",
                                    "id": "a2dfd19cac0b",
                                    "text": "table",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "note": "区别：\n\n|  | 内部表 | 外部表 |\n| --- | --- | --- |\n| 建表语句 | 外部表需要通过`external`关键字声明 |  |\n| 数据存储位置 | 内部表数据存储位置由 `hive.metastore.warehouse.dir`参数指定，默认存储在 HDFS 的 `/user/hive/warehouse/数据库名.db/表名/`  目录下 | 外部表数据的存储位置创建表时由 `Location` 参数指定； |\n| 删除表 | 删除元数据（metadata）和文件 | 只删除元数据（metadata） |\n\n外部表转换为内部表：\n```sql\nalter table student2 set tblproperties('EXTERNAL'='FALSE');\n```",
                                            "id": "bc71396a3a03",
                                            "text": "内部表和外部表",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "**分区表**：物理上表现为表文件夹的子文件夹。在大数据量的表中，使用分区字段作为where语句的过滤条件，可以避免全表扫描，大幅提高查询效率。\n\n对分区表的操作\n```sql\n# 创建分区表\ncreate table t1(...) partitioned by (col1 datatype [, col2 datatype ...]);\n\n# 查询分区中的数据\nselect * from emp_partition where deptno = 20;\n\n# 增加分区，若同时添加多个分区，用空格分隔\nalter table dept_partition add partition(month='201705') partition(month='201704');\n\n# 查看某表的分区\nshow partitions t1;\n```\n---\n\n**分桶表**：物理上表现为表文件夹的文件，比分区表粒度更细，根据指定列的哈希值，将数据存入不同的桶文件中。分桶表可以提高join效率：进行join的两个表，若事先根据关联列进行了分桶，可以避免shuffle过程，即sort merge bucket join；另外，分桶表还可以用于超大数据集的取样。\n\n\n创建方式：建表语句中添加`clustered by (empno) sorted by (empno asc) into 4 buckets`\n\n---\n\n分区表和分桶表常常可以结合使用，从而实现表数据在不同粒度上都得到合理的拆分。\n\n",
                                            "id": "79edc8ba76eb",
                                            "text": "分区表和分桶表",
                                            "layout": null
                                        },
                                        "children": [
                                            {
                                                "data": {
                                                    "note": "动态分区插入：向分区表插入数据时，只指定分区列而不指定具体分区值。\n\n动态分区插入时，分区列**必须存在并位于select语句的末尾**，若为多个分区字段，还需保持partition子句中的顺序。数据会根据分区列的值进入到不同的分区中。\n\n常用配置参数，一般无需改动：\n| 配置                                       | 默认值   | 说明                                                         |\n| ------------------------------------------ | -------- | ------------------------------------------------------------ |\n| `hive.exec.dynamic.partition`              | `true`   | 启用动态分区插入                                             |\n| `hive.exec.dynamic.partition.mode`         | `strict` | 严格模式下，用户必须至少指定一个静态分区，以防用户意外覆盖所有分区，在非严格模式下，允许所有分区都是动态的 |\n| hive.exec.max.dynamic.partitions.pernode | 100      | 允许在每个节点中创建的最大动态分区数         |\n| hive.exec.max.dynamic.partitions         | 1000     | 允许总共创建的最大动态分区数                                 |\n| hive.exec.max.created.files              | 100000   | 作业中所有节点创建的HDFS文件的最大数        |\n| hive.error.on.empty.partition           | false  | 如果动态分区插入生成空结果，是否抛出异常                     |\n\n\n语法：\n`insert overwrite table t1 partition(loc) select deptno, dname, loc from dept;`",
                                                    "id": "732ae2288da4",
                                                    "text": "动态分区",
                                                    "layout": null
                                                },
                                                "children": []
                                            }
                                        ]
                                    },
                                    {
                                        "data": {
                                            "note": "指定一个或多个倾斜列，以及经常出现的倾斜值，Hive 会自动将涉及到这些值的数据拆分为单独的文件。在查询时，如果涉及到倾斜值，它就直接从独立文件中获取数据，而不是扫描所有文件，**一定程度上提升了查询效率**。\n\n语法：`create table t1(...) skewed by (col1,...) on (v1,...,vn)`",
                                            "id": "7baeb12c8ceb",
                                            "text": "倾斜表",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "临时表仅对当前session可见，数据暂存在用户目录中，并在会话结束后删除。注意临时表不支持分区；\n\n语法：`create temporary table t1 ...`",
                                            "id": "b78adc43e2ac",
                                            "text": "临时表",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "Hive 中的视图和 RDBMS 中视图的概念一致，都是一组数据的逻辑表示，本质上是一次查询的结果集。视图不关联物理存储 (物化视图除外)。\n\n视图是只读的，不能被load/insert/alter。\n\n视图被创建后就固定了，视图参与查询时，会先被执行得到结果，再执行后续操作。\n\n删除基表不会同时删除视图，视图被创建后，基表或列不存在，执行视图时会报错。\n",
                                            "id": "1f9acbcdef10",
                                            "text": "视图view",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "`CREATE TABLE emp_copy AS SELECT * FROM emp WHERE deptno='20';`",
                                            "id": "6ec04f911413",
                                            "text": "CATS创建表",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "语法：\n\n```\nCREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name  --创建表表名\n   LIKE existing_table_or_view_name  --被复制表的表名\n   [LOCATION hdfs_path]; --存储位置\n```\n\n示例：\n\n```\nCREATE TEMPORARY EXTERNAL TABLE  IF NOT EXISTS  emp_co  LIKE emp\n```",
                                            "id": "d90936ff05b4",
                                            "text": "复制创建表",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "查看数据库：\n\n```\nDESCRIBE|Desc DATABASE [EXTENDED] db_name;  --EXTENDED 是否显示额外属性\n```\n\n查看表：\n\n```\nDESCRIBE|Desc [EXTENDED|FORMATTED] table_name --FORMATTED 以友好的展现方式查看表详情\n```",
                                            "id": "8603d18236cf",
                                            "text": "describe",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "**1. 查看数据库列表**\n\n```\n-- 语法\nSHOW (DATABASES|SCHEMAS) [LIKE 'identifier_with_wildcards'];\n\n-- 示例：\nSHOW DATABASES like 'hive*';\n```\n\nLIKE 子句允许使用正则表达式进行过滤，但是 SHOW 语句当中的 LIKE 子句只支持 `*`（通配符）和 `|`（条件或）两个符号。例如 `employees`，`emp*`，`emp*|* ees`，所有这些都能匹配到`employees`。\n\n**2. 查看表的列表**\n\n```\n-- 语法\nSHOW TABLES [IN database_name] ['identifier_with_wildcards'];\n\n-- 示例\nSHOW TABLES IN default;\n```\n\n**3. 查看视图列表**\n\n```\nSHOW VIEWS [IN/FROM database_name] [LIKE 'pattern_with_wildcards'];   --仅支持 Hive 2.2.0 +\n```\n\n**4. 查看表的分区列表**\n\n```\nSHOW PARTITIONS table_name;\n```\n\n**5. 查看表/视图的创建语句**\n\n```\nSHOW CREATE TABLE ([db_name.]table_name|view_name);\n```",
                                            "id": "6702dad184fa",
                                            "text": "show",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "data": {
                            "id": "74d50b858f53",
                            "text": "DML",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "note": "```sql\nSELECT [ALL | DISTINCT] select_expr, select_expr, ...\nFROM t1\n[WHERE where_condition]\n[GROUP BY col_list]\n[ORDER BY col_list]\n[CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list]\n]\n[LIMIT number]\n```\n\n语法与sql语法基本一致，只列出一些重点和注意点。",
                            "id": "123284b38b37",
                            "text": "查询",
                            "layout": null
                        },
                        "children": [
                            {
                                "data": {
                                    "note": "hive支持limit n的语法，但不支持limit n offset m或者limit m,n【在sql中表示从m开始的n条数据，m初始值为0】.",
                                    "id": "3450a2df7ab5",
                                    "text": "Limit",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "与MySQL不同：\n- hive中的where子句不能使用字段别名。\n- 表示一定成立的条件时，hive中不能使用`where 1`, 可以使用`where 1=1`;",
                                    "id": "6eedfe77e5c4",
                                    "text": "where",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "普通的`like`使用同MySQL，如：`select * from emp where like '2%';`\n\n可以使用`rlike`结合**Java的正则表达式**进行匹配，如：`select * from emp whrere sal rlike '[2]';`\n\nhive中的`show ... like ...`语句，需要使用Java正则表达式进行匹配，如：\n```sql\nhive (default)> show functions like '*day*';  --使用regex查找到结果\nOK\ntab_name\nday\ndayofmonth\ndayofweek\nfloor_day\nlast_day\nnext_day\nTime taken: 0.015 seconds, Fetched: 6 row(s)\nhive (default)> show functions like '%day%';  --使用普通like语法查找为空\nOK\ntab_name\nTime taken: 0.015 seconds\n\n```",
                                    "id": "cd08ab259f07",
                                    "text": "Like",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "distinct以其后的**所有列**为key进行去重。\n\nhive中的distinct必须出现在select的最前面，否则报错：\n```sql\nselect house_id, distinct id, user_id, salesman_id from test.trip_tmp;\nNoViableAltException(96@[80:1: selectItem : ......\n```\n\ndistinct不能和聚合函数并列使用，也不能和group by一起使用，否则报错。但可以在聚合函数中使用distinct，若有多个聚合函数都是用了distinct，这些字段必须一致。\n```sql\nselect distinct id, user_id, salesman_id, count(house_id) from trip_tmp;\nFAILED: SemanticException [Error 10128]: Line 1:42 Not yet supported place for UDAF 'count'\n\nselect count(distinct id) from trip_tmp;\nOK\n6\nTime taken: 4.775 seconds, Fetched: 1 row(s)\n```\n\ndistinct不能过滤NULL值。\n\ngroup by可以替代distinct，两者底层实现一致，一般推荐使用group by。",
                                    "id": "7121cb41a828",
                                    "text": "distinct",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "5f949430ef91",
                                    "text": "join",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "note": "Hive3.x和Spark 3.x 版本中，一共支持以下七种 Join 类型：\n\n- INNER JOIN：返回左右表同时满足关联条件的结果\n- CROSS JOIN：笛卡尔积，返回结果行数为`m * n`\n- LEFT [OUTER] JOIN：左表数据全部返回，右表悬浮字段补NULL值\n- RIGHT [OUTER] JOIN：右表数据全部返回，左表悬浮字段补NULL值\n- FULL [OUTER] JOIN：左右表每行数据都被返回，任意一边关联不上时补NULL值\n- LEFT SEMI JOIN：只返回inner join结果中属于左表的字段，可以用in/exists改写，三者在Hive 3.x中执行计划完全相同。\n- LEFT ANTI JOIN：返回左表中不存在于inner join结果集中的行",
                                            "id": "7ea07ed7b314",
                                            "text": "7种join类型",
                                            "layout": null
                                        },
                                        "children": [
                                            {
                                                "data": {
                                                    "note": "* 没有连接条件on：即使有where，也是先笛卡尔积再用where过滤\n* 连接条件无效\n* 显式使用cross join",
                                                    "id": "0f7e6bc288cf",
                                                    "text": "导致笛卡尔积的情况",
                                                    "layout": null
                                                },
                                                "children": []
                                            }
                                        ]
                                    },
                                    {
                                        "data": {
                                            "note": "| Type                   | Approach                                                     | Pros                                         | Cons                                                 |\n| ---------------------- | ------------------------------------------------------------ | -------------------------------------------- | ---------------------------------------------------- |\n| Shuffle Join           | join keys are shuffled using map/reduce and joins performed join side | 不受数据大小和布局的限制                     | 性能开销最大且最慢                                   |\n| Broadcast Join         | small tables are loaded into memory in all nodes, mapper scans through the large table and joins | very fast, single scan through largest table | all but one table must be small enough to fit in RAM |\n| Sort-Merge Bucket Join | mappers table advantage of co-location of keys to do efficient joins | very fast for tables of any size             | Data must be sorted and bucketed ahead of time       |\n\n",
                                            "id": "a15cffe6d935",
                                            "text": "3种join策略",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            },
                            {
                                "data": {
                                    "id": "fe6ec6b69b98",
                                    "text": "分组与排序",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "note": "与MySQL不同：\n- 使用了group by时，select后面的非聚合函数字段，必须出现在group by中；\n- hive/oracle的where/ group by/ having都不能使用别名。\n\ngroup by 一定会触发reduce，其速度与reducer数量有关，输出文件为reducer个数，可通过`mapreduce.job.reduces`调整reducer个数。",
                                            "id": "8f17788c8111",
                                            "text": "group by",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "全局排序，只有一个reducer。位于select语句的末尾，执行顺序也是最后。\n> 在严格模式下(`hive.mapred.mode= strict`)，后面还必须跟一个`limit n`子句。",
                                            "id": "2df05c1079b6",
                                            "text": "order by",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "`distribute by`类似MR的partition，根据指定列的hash值，将数据行发往不同分区，该列hash值相同的行一定到达同一个reducer，分区数为reducer数，可以结合`sort by`在分区内排序。\n\n>`group by`与`distribute by`的区别：都是按key的hash值将数据行发往不同的reducer。唯一不同的是，distribute by只是单纯地将数据按key分区，而group by必须对分区的数据进行聚合操作。",
                                            "id": "7e3cfd708d83",
                                            "text": "distribute by",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "* order by 保证全局有序，因此只能使用一个reducer进行排序。\n* sort by只保证分区内有序，不保证全局有序，分区数等同于reducer数。",
                                            "id": "b332e2fd6c78",
                                            "text": "sort by",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "当distribute by和sort by字段相同，且sort by排序规则是ASC时(cluster by只能用于升序)，可以使用cluster by代替。",
                                            "id": "81bb8b5e307b",
                                            "text": "cluster by",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "data": {
                            "id": "5194c375d2c9",
                            "text": "常用函数",
                            "layout": null
                        },
                        "children": [
                            {
                                "data": {
                                    "note": "```sql\n--查看系统自带函数\nshow functions;\n--查看某个函数的用法\ndesc function [extended] upper;  --extended显示详细用法\n```",
                                    "id": "3c429f03afd8",
                                    "text": "查看内置函数",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "若value为null，返回默认值，否则返回value",
                                    "id": "0f94107d2d6a",
                                    "text": "nvl(val, default)",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "55b63381dd32",
                                    "text": "<span style=\"font-size: inherit;\">case when con1 then ... when con2 then ... else ... end</span><br>",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "若表达式exp1为true,返回val1，否则返回val2",
                                    "id": "400895789ef4",
                                    "text": "if(exp1, val1, val2)",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "",
                                    "id": "a796afd1acc7",
                                    "text": "concat",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "note": "将任意个数的输入拼接为字符串，元素可以是string、数字或某列\n\n> 注：\n- 存在元素为null时，返回null\n- mysql的concat只能接收两个参数\n\n示例：\n\n```sql\nselect concat('*','1','2',3); --> *123\nselect concat('*','1','2',3,null); --> NULL\n```",
                                            "id": "74e99b80cbea",
                                            "text": "concat(str1/col1, str2/col2, ...)",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "用分隔符将后续元素或集合中的所有元素拼接\n\n> 注：\n- 若元素为null或空字符串，会被跳过。\n- 若分隔符为null，返回null\n\n示例：\n\n```sql\nselect concat_ws(null,'1','2','4');  --> NULL\nselect concat_ws('*','1','2',null,'4'); --> 1*2*4\n```",
                                            "id": "1ce68ba684ce",
                                            "text": "concat_ws(separator, str1, str2, ...)<br>concat_ws(separator, 集合)<br>",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            },
                            {
                                "data": {
                                    "note": "设person表的xjson字段有数据：\n\n`[{\"name\":\"王二狗\",\"age\":\"25\"},{\"name\":\"李狗嗨\",\"age\":\"47\"}]`\n\n```\n# 取json数组的某个元素\nSELECT get_json_object(xjson,\"$.[0]\") FROM person;\n{\"name\":\"王二狗\",\"age\":\"25\"}\n# 取json对象的某个属性值：\nSELECT get_json_object(xjson,\"$.[0].age\") FROM person;\n25\n```",
                                    "id": "8c5ec53973ea",
                                    "text": "get_json_object()",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "结合group by，将同组数据的指定列收集为集合，set需去重，list不需去重。\n\n> 指定的列只能是进基本数据类型，不能是集合数据类型\n\n示例：\n\n```\n>>>cat /opt/data/person.txt\n孙悟空  A\n猪八戒  B\n唐僧    A\n沙僧    B\n```\n\n```sql\ncreate table person(\nname string,\nblood_type string)\nrow format delimited fields terminated by \"\\t\";\nload data local inpath \"/opt/data/person.txt\" into table person;\n\nselect concat_ws('|',collect_set(name)) from person group by blood_type;\n孙悟空|猪八戒\n唐僧|沙僧\n```\n\n",
                                    "id": "6814114bdfe1",
                                    "text": "collect_set(col) / collect_list(col)<br>",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "将表中某个集合类型拆分为单个元素。\n\n示例：\n\n```sql\nhive> select explode(array(1,2,3)) as num;\nhive> select explode(map(\"name\",\"xiaoming\",\"age\",23)) as (k,v);\n```",
                                    "id": "0a44ac8ef59e",
                                    "text": "explode(col)",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "结合`split`, `explode`等`UDTF`使用，将集合列拆分为一个虚拟表，并将虚拟表各行与原表对应行关联，将原表爆炸N倍。\n\n语法：\n\n`select col1,col2 from base_tbl lateral view udtf(col) tbl_alias as col2;`\n\n示例：\n\n```sql\ncreate table pageAds(pageid string, adid_list array<int>);\ninsert into pageAds values (\"front_page\", array(1,2)), (\"contact_page\",array(3,4));\nSELECT pageid, adid FROM pageAds LATERAL VIEW explode(adid_list) adTable AS adid;\n\npageid  adid\nfront_page      1\nfront_page      2\ncontact_page    3\ncontact_page    4\n```",
                                    "id": "54ae90b7c47a",
                                    "text": "lateral view",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "8ad64d047a06",
                                    "text": "自定义函数",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "note": "1.创建Maven工程HiveUDF\n2.导入依赖\n```xml\n<dependencies>\n        <dependency>\n             <groupId>org.apache.hive</groupId>\n             <artifactId>hive-exec</artifactId>\n             <version>3.1.2</version>\n        </dependency>\n</dependencies>\n```\n3.创建一个类，继承UDF类并重写`evaluate()`方法\n```java\npackage com.example.hive;\nimport org.apache.hadoop.hive.ql.exec.UDF;\n \npublic class Lower extends UDF {\n    public String evaluate (final String s) {\n        if (s == null) {\n             return null;\n        }\n        return s.toLowerCase();\n    }\n}\n```\n4.打包为jar包，上传到集群指定目录，如：/opt/jars/udf.jar\n5.将该jar包添加到hive的classpath\n```sql\nhive> add jar /opt/jars/udf.jar;\n```\n6.创建临时函数与Java class关联\n```sql\nhive> create temporary function mylower as \"com.example.hive.Lower\";\n```\n7.在hive中使用自定义函数\n```sql\nhive> select ename, mylower(ename) as lowername from emp;\n```",
                                            "id": "f654e3c5e559",
                                            "text": "udf 一进一出",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "id": "1e12291ecfe2",
                                            "text": "udaf 多进一出",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "0.需求\n自定义一个 UDTF 实现功能：使用分割符将字符串切割成多个单词，例如：\n\n```\nhive(default)> select myudtf(\"hello,world,hadoop,hive\", \",\");\nhello\nworld\nhadoop\nhive\n```\n\n1.代码实现\n继承GenericUDTF类，并重写`initialize()`，`process()`，`close()`方法\n\n```\npackage hive;\n\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\nimport org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class UDTF extends GenericUDTF {\n    private ArrayList<String> outList = new ArrayList<>();\n\n    @Override\n    public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException {\n        //1.定义输出数据的列名和类型\n        List<String> fieldNames = new ArrayList<>();\n        List<ObjectInspector> fieldOIs = new ArrayList<>();\n        //2.添加输出数据的列名和类型\n        fieldNames.add(\"lineToWord\");\n        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);\n        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);\n    }\n\n    @Override\n    public void process(Object[] args) throws HiveException {\n        //1.获取原始数据\n        String arg = args[0].toString();\n        //2.获取数据传入的第二个参数，此处为分隔符\n        String splitKey = args[1].toString();\n        //3.将原始数据按照传入的分隔符进行切分\n        String[] fields = arg.split(splitKey);\n        //4.遍历切分后的结果，并写出\n        for (String field : fields) {\n            //集合为复用的，首先清空集合\n            outList.clear();\n            //将每一个单词添加至集合\n            outList.add(field);\n            //将集合内容写出\n            forward(outList);\n        }\n    }\n\n    @Override\n    public void close() throws HiveException {\n    }\n}\n```",
                                            "id": "d7f9bb897971",
                                            "text": "udtf 一进多出",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            },
                            {
                                "data": {
                                    "note": "窗口函数的执行顺序只早于`distinct`和`order by`，是**对前面操作的结果进行开窗分析**\n\n> 开窗函数只为每一行增加一列，不影响先前结果的行数。\n\n格式：`func() over([partition by ... order by ... [开窗子句]])`\n\n- func()：分为聚合函数和序列函数\n- over()：用于指定分析窗口的大小",
                                    "id": "cf858640e329",
                                    "text": "窗口函数",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "note": "**聚合函数**：支持group by中的大多数聚合函数，但此时是对窗口内的数据行聚合\n\n**序列函数**：与排序相关，使用前必须先使用`order by`排序\n* `ntile(N int)`：将分区并排序的数据切成N等份，每行该列的值为切片的编号(从1开始)。不能整除时，前面的切片会多一行。\n\n* `row_number() / rank() / dense_rank()`：返回排序值，`row_number()`不允许相等排名，如`1,2,3,4`；`rank()`允许相等排名，会在名次中留下空位，如`1,1,3,4`；`dense_rank()`允许相等排名，不会在名次中留下空位，如`1,1,2,3`。\n\n> 注意：使用排序函数时，null是最大的，降序排列时会排在最前面，影响排序结果。可以这样： `rank() over(order by score desc nulls last)`\n\n* `lag(col,n,default)`：返回当前行后退n行的col列，没有则返回default，未设默认值则返回null\n* `lead(col,n,default)`：返回当前行前进n行的col列，没有则返回default，未设默认值则返回null\n* `first_value`：返回分组内排序后的第一个值\n* `last_value`：返回分组内排序后截至当前行的最后一个值",
                                            "id": "b8724153ae1a",
                                            "text": "func()",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "**partition by**：将数据**按指定列分区**，可看做over()中的distribute by，一个分区即一个窗口。\n\n**order by**：对分区内数据**按指定列排序**。order by 子句在`row_number(),lead(),lag()`等**序列函数场景中不可或缺**。\n\n**开窗子句**：`ROWS BETWEEN ... AND ...`，必须先使用`order by`排序，**在分区内进一步缩小窗口大小**，**只用于聚合函数之后**。\n```\n`CURRENT ROW`：当前行\n`n PRECEDING`：往前n行数据\n`n FOLLOWING`：往后n行数据\n`UNBOUNDED PRECEDING`：起点\n`UNBOUNDED FOLLOWING`：终点\n\n示例：\n`ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`（从起点到当前行）\n`ROWS BETWEEN 2 PRECEDING AND 1 FOLLOWING`（往前2行到往后1行）\n`ROWS BETWEEN 2 PRECEDING AND CURRENT ROW`（往前2行到当前行）\n`ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING`（当前行到终点）\n```\n\n> 空的over()中表示对全体数据开窗。",
                                            "id": "d509885cea13",
                                            "text": "over()可接的子句",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            },
                            {
                                "data": {
                                    "note": "```\n#1. current_date() 返回当前日期\nhive> select current_date(); --> 2021-03-13\n\n#2. current_timestamp()返回当前日期和时间，等价于MySQL的now()\nhive (default)> select current_timestamp(); --> 2021-03-13 15:53:02.616\n\n#3. date_add()/date_sub() 日期的加减\nhive (default)> select date_add(current_date(),90); --> 2021-06-11\nhive (default)> select date_sub(\"2021-03-01\",1); --> 2021-02-28\n\n#4. datediff() 求日期差\nSELECT datediff(CURRENT_DATE(), \"1990-06-04\"); --> 11240\n\n#5. next_day(start_date, day_of_week)  返回今日后的下一个周几\nhive (default)> select current_date(), next_day(current_date(),'sun');\n2021-03-13      2021-03-14\n\n#6. date_format()根据格式整理日期\nhive (default)> select date_format(current_date(),'yyyyMMdd'); --> 20210313\n\n#7. last_day()当月最后一天的日期\nhive (default)> select last_day(current_date()); --> 2021-03-31\n\n#8. to_date()\nselect to_date('2015-04-02 13:34:12'); --> 2015-04-02\n\n#9. from_unixtime()：转化unix时间戳到当前时区的时间格式\nselect from_unixtime(1323308943,’yyyyMMdd’); --> 20111208\n\n#10. unix_timestamp()：获取指定datetime的unix时间戳\nselect unix_timestamp(); --> 1430816254\nselect unix_timestamp('2015-04-30 13:51:20'); --> 1430373080\n#11. year() / month() / day() / hour() / minute() / second()\n\n#12. weekofyear()：返回日期在当前周数\nselect weekofyear('2015-05-05 12:11:1'); --> 19\n```",
                                    "id": "4680ff3f4330",
                                    "text": "日期相关",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "5da5b7d45ed1",
                                    "text": "其他",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "id": "d854d1a4c953",
                                            "text": "regexp_replace(string, 'exp1', 'exp2')",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "returns the substring of str that starts at pos and is of length len, index starts at 1",
                                            "id": "804c7e93f39d",
                                            "text": "substr(str, pos[, len])",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "若substr为str的字串，返回substr在str中的起始index,从1开始，失败返回0",
                                            "id": "64ad2735eed5",
                                            "text": "instr(str, substr)",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "data": {
                            "id": "3bd4e0e400e1",
                            "text": "压缩与存储",
                            "layout": null
                        },
                        "children": [
                            {
                                "data": {
                                    "note": "![image.png](https://yeluoqianqiu.github.io/assets/%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8.png)\n\n列式存储的优势：\n\n- 列裁剪：查询时只读取需要的列，可以降低I/O消耗。\n- 高效压缩：每一列数据时同构的，可以针对不同的数据类型使用更高效的数据压缩算法，减小文件体积，降低读取I/O消耗。",
                                    "id": "352984e8b875",
                                    "text": "列式存储",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "8b684b13e108",
                                    "text": "存储格式",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "note": "默认格式，纯文本，不压缩",
                                            "id": "4d23847e33d9",
                                            "text": "TextFile",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "二进制文件，行存储，可分割，可压缩，将数据以`<key,value>`的形式序列化到文件中。继承自Hadoop中的SequenceFile，不过它的 key 为空，使用 value 存放实际的值，以避免 MR 在map阶段进行不必要的排序",
                                            "id": "30094ca21080",
                                            "text": "SequenceFile",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "列式存储，树状结构，对嵌套数据友好。",
                                            "id": "b56567d03105",
                                            "text": "parquet",
                                            "layout": null
                                        },
                                        "children": [
                                            {
                                                "data": {
                                                    "note": "Parquet灵感来自google Dremel，嵌套格式类似protocol buffers，每一个数据模型的schema包含多个字段，每一个字段有三个属性：重复次数、数据类型和字段名。\n\n- 重复次数：required (1)，repeated (0, n)，optional (0, 1)。\n- 数据类型：group(复杂类型)、primitive(基本类型)。\n\n例如Dremel中提供的Document的schema示例，它的定义如下：\n\n```text\nmessage Document {\n  required int64 DocId;\n  optional group Links {\n    repeated int64 Backward;\n    repeated int64 Forward; \n  }\n  repeated group Name {\n    repeated group Language {\n      required string Code;\n      optional string Country; \n     }\n    optional string Url; \n  }\n}\n```\n\n可以把这个Schema转换成树状结构，根节点可以理解为repeated类型，如图：\n\n<img src=\"https://yeluoqianqiu.github.io/assets/parquet%E6%A0%91%E7%8A%B6%E7%BB%93%E6%9E%84.png\" style=\"zoom: 67%;\" />\nParquet中没有Map、Array这样的复杂数据结构，但是可以通过repeated和group组合来实现。",
                                                    "id": "561a7ae2199c",
                                                    "text": "数据模型",
                                                    "layout": null
                                                },
                                                "children": []
                                            },
                                            {
                                                "data": {
                                                    "note": "Parquet文件是自解析的，文件中包括该文件的数据和元数据。Parquet文件结构中的概念：\n\n- **行组(Row Group)**：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，概念类似于ORC的stripe。\n- **列块(Column Chunk)**：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，**不同的列块可能使用不同的压缩算法**。\n- **页(Page)**：每一个列块划分为多个页，页是**最小的编码的单位**，在同一个列块的不同页可能使用不同的编码方式。\n\n通常按照HDFS块大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个块，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。\n\nParquet的文件结构图：\n\n![](https://yeluoqianqiu.github.io/assets/parquet%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84%E5%9B%BE.png)\n\n相关概念：\n\n- Magic Number：位于文件首尾，用于校验它是否是一个Parquet文件。\n- Footer：记录文件的version、schema和所有行组、列、页的元数据\n- Footer length：记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量",
                                                    "id": "f967ff751831",
                                                    "text": "文件结构",
                                                    "layout": null
                                                },
                                                "children": []
                                            }
                                        ]
                                    },
                                    {
                                        "data": {
                                            "note": "RC：FaceBook开源，列式存储，首先将表分为几个行组，对每个行组内的数据按列存储\n\nORC：对RCFile在压缩编码和查询性能方面进行了优化，对扁平数据压缩效率奇高。列式存储、支持ACID、支持struct/map/array等复杂类型(实现较复杂)。",
                                            "id": "2587439411fa",
                                            "text": "rc/orc",
                                            "layout": null
                                        },
                                        "children": [
                                            {
                                                "data": {
                                                    "note": "和Parquet不同，ORC原生是不支持嵌套数据格式的，而是通过对复杂数据类型特殊处理的方式实现嵌套格式的支持，例如对于如下的hive表：\n\n```sql\nCREATE TABLE `orcStructTable`(\n  `name` string,\n  `course` struct<course:string,score:int>,\n  `score` map<string,int>,\n  `work_locations` array<string>);\n```\n\nORC格式会将其转换成如下的树状结构：\n\n![](https://yeluoqianqiu.github.io/assets/orc%E6%A0%91%E7%8A%B6%E7%BB%93%E6%9E%84.png)\n\n可以看到，这个schema包含10个column，其中包含了复杂类型列和原始类型的列，STRUCT可能有多个孩子节点，每个节点包括它的一个成员变量，MAP有两个孩子节点，分别为key和value，LIST包含一个孩子节点，类型为该LIST的成员类型。每一个Schema树的根节点为一个Struct类型，所有的column按照树的中序遍历顺序编号。\n\nORC只需要存储schema树中叶子节点的值，而中间的非叶子节点只是做一层代理，它们只需要负责孩子节点值得读取，只有真正的叶子节点才会读取数据，然后交由父节点封装成对应的数据结构返回。",
                                                    "id": "4d87dff3d829",
                                                    "text": "数据模型",
                                                    "layout": null
                                                },
                                                "children": []
                                            },
                                            {
                                                "data": {
                                                    "note": "ORC也是自解析的，元数据使用protocol buffers序列化。ORC原生是不支持嵌套数据类型的，而是通过对复杂数据类型特殊处理的方式实现嵌套格式的支持。\n\n![image.png](https://yeluoqianqiu.github.io/assets/image-20210526001202-36a6s9n.png)\n\nStrip：相当于parquet中的row group，**每个stripe一般为HDFS的块大小**。每个Stripe由三部分组成：\n\n- Index Data：轻量级的index，记录某行的各字段在Row Data中的offset。最小单位为row group(不要混淆)，默认是一万行。**strip按照row group分块，块内按列存储。**\n- Row Data：存储数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。\n- Stripe Footer：存的是各个Stream的类型，长度等信息。\n\nstream：一个stream表示文件中一段有效的数据，包括索引和数据两类，不同类型的stream可使用不同的压缩算法。\n\n- 索引stream：保存每一个row group的位置和统计信息\n- 数据stream包括多种类型的数据，具体需要哪几种是由该列类型和编码方式决定。\n\nFile Footer：存储了每个Stripe的行数、每个Column的数据类型信息等\n\nPostScript：位于文件尾部，记录了整个文件的压缩类型以及FileFooter的长度信息等。\n\n在ORC文件中保存了三个层级的统计信息，分别为文件级别、stripe级别和row group级别的，他们都可以用来根据Search ARGuments（谓词下推条件）判断是否可以跳过某些数据，在统计信息中都包含成员数和是否有null值，并且对于不同类型的数据设置一些特定的统计信息。\n\nORC文件是从后往前读的：会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe。",
                                                    "id": "311e3b028bef",
                                                    "text": "文件结构",
                                                    "layout": null
                                                },
                                                "children": []
                                            }
                                        ]
                                    },
                                    {
                                        "data": {
                                            "note": "parquet对嵌套数据更友好，以树状结构存储数据\n\norc对扁平数据更友好，对扁平数据压缩率奇高",
                                            "id": "e7565aec23e4",
                                            "text": "parquet与orc的选择",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            },
                            {
                                "data": {
                                    "note": "在Hadoop中常用的几种压缩格式：bzip2, gzip, lzo，snappy。主要特性对比如下：\n\n| 压缩格式 | codec类     | 算法    | 扩展名  | 可切分       | Hadoop自带 |\n| -------- | ----------- | ------- | ------- | ------------ | ---------- |\n| bzip2    | Bzip2Codec  | bzip2   | .bz2    | 是           | 是         |\n| gzip     | GzipCodec   | deflate | .gz     | 否           | 是         |\n| lzo      | LzopCodec   | lzo     | .lzo    | 是[需建索引] | 否         |\n| snappy   | SnappyCodec | snappy  | .snappy | 否           | 否         |\n\n其性能对比如下：\n\n| 压缩格式 | 压缩比 | 压缩速率 | 解压速率 |\n| -------- | ------ | -------- | -------- |\n| bzip2    | 13.2%  | 2.4MB/s  | 9.5MB/s  |\n| gzip     | 13.4%  | 21MB/s   | 118MB/s  |\n| lzo      | 20.5%  | 135MB/s  | 410MB/s  |\n| snappy   | 22.2%  | 172MB/s  | 409MB/s  |\n\n注：\n\n1.所有算法的实现均在`org.apache.hadoop.io.compress`包下面；如果原始数据已经压缩(如jpeg)，则不建议再压缩\n2.gzip压缩：\n   - 优点：**压缩率高**，压缩/解压速度还行，hadoop本身支持；\n   - 缺点：不支持文件切分\n   - 应用场景：高压缩比，适合用于存储冷数据\n\n3.lzo压缩：\n   - 优点：压缩/解压速度较快，合理的压缩率，**支持切分大文件**(需要建索引，且指定inputformat为lzo，文件修改后需要重新建索引)\n   - 缺点：压缩率较低，hadoop本身不支持，需要安装；\n   - 应用场景：处理大文件，需要切块时\n\n4.snappy压缩：\n   - **优秀的压缩/解压速度**\n   - 不支持文件切分，压缩率较低，hadoop本身不支持，需要安装；\n   - 应用场景：当mapreduce作业的map输出的数据比较大的时候，作为map到reduce的中间数据的压缩格式；或者作为一个mapreduce作业的输出和另外一个mapreduce作业的输\n   - **Snappy只是不能切分TextFile，可以切分SequenceFile/Avro/Parquet/Orc文件**",
                                    "id": "e1b5ae8d8827",
                                    "text": "压缩格式",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "在实际的项目开发当中， hive 表的数据存储格式一般选择： orc 或 parquet 。压缩方式一般选择 snappy ， lzo。",
                                    "id": "7a1a43bb170a",
                                    "text": "总结",
                                    "layout": null
                                },
                                "children": []
                            }
                        ]
                    },
                    {
                        "data": {
                            "id": "08991a1b7bdb",
                            "text": "导入与导出",
                            "layout": null
                        },
                        "children": [
                            {
                                "data": {
                                    "note": "方式1：\n`LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] \nINTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]`\n\n- local 表示本地文件，缺省则为HDFS文件\n- 加载到分区表必须指定分区\n\n\n方式2：import命令从HDFS导入\n```shell\nhive (default)> import table student from '/user/hive/warehouse/export/student';\n```\n\n方式3：查询结果导入\n`insert overwrite table t1 select ...`\n\n方式4：建表时导入\n`create table t1 as select ...`",
                                    "id": "6d0c9704b85f",
                                    "text": "导入",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "方式1：insert到本地或hdfs\n```sql\n\ninsert overwrite [local] directory 'path' [row format delimited FIELDS TERMINATED BY '\\t' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':'] [stored as file_format] select * from src;\n```\n\n方式2：Hadoop命令直接获取表文件\n```shell\n\nhive (default)> dfs -get /user/hive/warehouse/student/student.txt\n/opt/module/datas/export/student3.txt;\n```\n\n方式3：hive -e/-f执行sql语句或脚本并重定向到文件\n\n```shell\n[atguigu@hadoop102 hive]$ bin/hive -e 'select * from default.student;' >\n /opt/module/hive/datas/export/student4.txt;\n```\n\n方式4：export命令到HDFS\n```sql\n(hive default)> export table student to '/user/hive/warehouse/export/student';\n```\n\n方式5：使用Sqoop等工具",
                                    "id": "3b21935465c1",
                                    "text": "导出",
                                    "layout": null
                                },
                                "children": []
                            }
                        ]
                    },
                    {
                        "data": {
                            "note": "**0.Explain查看执行计划**\n\n生产环境中的任务一般执行时间都是几十分钟到几个小时，不可能每次都通过任务执行完成时间来评估任务的执行效率，可以通过explain查看执行计划来分析任务的执行效率。\n\n基本语法：\n\n`EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query`\n\n* extended：显示详细执行计划\n* dependency：查看依赖的输入表、分区等信息\n\n案例实操：\n\n```sql\nexplain select * from emp;\n--没有生成MR，使用了Fetch Operator\n\nexplain select deptno, avg(sal) avg_sal from emp group by deptno;\n--生成了MR，使用了Map Operator Tree, Reduce Operator Tree, Select Operator, File Output Operator和Fetch Operator\n```\n\n**1.建表优化**\n\n分区表、分桶表、合适的文件存储格式和压缩格式\n\n**2.对中间和最终结果数据启用压缩**\n\n```sql\nset hive.exec.compress.intermediate=true;\nset hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;\nset hive.intermediate.compression.type=BLOCK;\nset hive.exec.compress.output=true;\n```\n\n**3.尽早过滤**\n\n- 空值去除与转换：空值可去则去，不可去除则转换为不影响结果的值。\n- 分区裁剪和列裁剪：查询时只指定必要的列和分区，避免全表扫描读取无关数据，减少内存压力和计算耗时\n- 先过滤再join：类似`a join b on condition where ... `的查询语句，会先join再过滤where，可以用where过滤后再进行join，优化join的执行效率。(现在即使不自己写子查询，谓词下推也会优化为where先过滤)\n\n**4.group by开启map预聚合和负载均衡**\n\n- 开启map端预聚合：`set hive.map.aggr=true;`\n- 数据倾斜时：`set hive.groupby.skewindata=true;`相当于打随机前后缀，会多一个MR任务将之前进入同一个节点的数据打散到多个节点，预处理之后再发往同一个Reduce中进行最终的聚合，此法需注意权衡性能的得失\n\n**5.向量化执行**\n\nHive中的向量化查询执行大大减少了典型查询操作（如scan, filter, aggregate和join)的CPU使用率。\n\n向量化执行将单行运算转换为批量运算(默认1024行一批，即一组列向量)，充分利用CPU。\n\n要使用向量化执行，必须以ORC格式存储数据，并设置：\n\n```sql\nset hive.vectorized.execution.enabled=true;\nset hive.vectorized.execution.reduce.enabled=true;\n```\n\n最新版本的Hive默认开启向量化执行。\n\n**6.谓词下推**\n\n默认生成的执行计划会在可见的位置执行过滤器，但在某些情况下，某些过滤器表达式可以被推到更接近首次看到此特定数据的运算符的位置。\n\n比如下面的查询：\n\n```sql\nselect\n    a.*,\n    b.* \nfrom\n    a join b on (a.col1 = b.col1)\nwhere a.col1 > 15 and b.col2 > 16\n```\n\n如果没有谓词下推，则在完成JOIN处理之后将执行过滤条件**(a.col1> 15和b.col2> 16)**。因此，在这种情况下，JOIN将首先发生，并且可能产生更多的行，然后在进行过滤操作。\n\n使用谓词下推，这两个谓词**(a.col1> 15和b.col2> 16)**将在JOIN之前被处理，因此它可能会从a和b中过滤掉连接中较早处理的大部分数据行，因此，建议启用谓词下推。\n\n通过将hive.optimize.ppd设置为true可以启用谓词下推(默认开启)。\n\n```text\nSET hive.optimize.ppd=true\n```\n\n**7.基于成本优化CBO**\n\nHive在提交最终执行之前会优化每个查询的逻辑和物理执行计划。基于成本的优化会根据查询成本进行进一步的优化，从而可能产生不同的决策：比如如何决定JOIN的顺序，执行哪种类型的JOIN以及并行度等。\n\n可以通过设置以下参数来启用基于成本的优化。\n\n```text\nset hive.cbo.enable=true;\nset hive.compute.query.using.stats=true;\nset hive.stats.fetch.column.stats=true;\nset hive.stats.fetch.partition.stats=true;\n```\n\n可以使用统计信息来优化查询以提高性能。基于成本的优化器（CBO）还使用统计信息来比较查询计划并选择最佳计划。通过查看统计信息而不是运行查询，效率会很高。\n\n收集表的列统计信息：\n\n```text\nANALYZE TABLE mytable COMPUTE STATISTICS FOR COLUMNS;\n```\n\n查看my_db数据库中my_table中my_id列的列统计信息：\n\n```text\nDESCRIBE FORMATTED my_db.my_table my_id\n```\n\n**8.join优化**\n\n**小表join大表：使用Map Join**\n\n普通的join是在reduce端进行的，而在小表join大表时，可以启用Map Join**把小表加载到每个mapper的内存** ，在map端进行join，没有了reduce，避免了shuffle。\n\nmap join 原理：\n\n1. 小表所在节点将小表数据转换为一个**HashTable** 文件，然后分发到各个map节点的内存中，形成**DistributeCache**\n2. 各mapper扫描大表，每一条记录去和DistributeCache进行关联，并直接输出结果，总输出文件的个数为map task的个数。\n\n![](https://yeluoqianqiu.github.io/assets/20210714005658.jpeg)\n\n开启map join：\n\n```sql\nset hive.auto.convert.join = true; //默认为true\n\n--小表的阈值设置（默认25M）\nset hive.mapjoin.smalltable.filesize=25000000;\n```\n\n早期版本中，需要将小表放在左边，大表放在右边，才会启用map join优化，新版hive中小表JOIN大表和大表JOIN小表已经没有区别了。\n\n显式使用map join：\n\n```sql\ninsert overwrite table res\nselect /*+mapjoin(s)*/ b.id, ...\nfrom bigtable  b\njoin smalltable  s\non s.id = b.id;\n```\n\n**大表join大表：使用sort merge bucket join**\n\n两个表join的时候，小表不足以放到内存中，但是又想用map side join这个时候就要用到bucket Map join。两个表根据**join key**创建为分桶表，**较小表的桶数应是较大表桶数的整数倍**。两表join时，小表依然复制到所有节点，Map join的时候，小表的每一组bucket加载成hashtable，与对应的一个大表bucket做局部join，这样每次只需要加载部分hashtable就可以了。\n\n**9.合理设置map和reduce个数**\n\n几个问题：\n\n- 是不是map数越多越好？\n\n不是。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。\n\n- 是不是保证每个map处理接近128m的文件块，就高枕无忧了？\n\n不一定。比如有一个127m的文件，正常会用一个map去完成，但**这个文件只有一个或者两个小字段，却有几千万的记录** ，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。\n\n- reduce是不是越多越好？\n\n过多的启动和初始化reduce也会消耗时间和资源；另外，**有多少个** **reduce** **，就会有多少个输出文件** ，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；\n\n因此需要合理设置map和reduce的个数。\n\n合理设置Map数：\n\nmap task的数量与参数`mapreduce.job.maps`无关(MR 2.x之后该参数无任何作用)，而是**与InputSplits大小有关**，计算公式为`computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))`，设置maxSize小于blocksize，可以增加map task的个数，而设置minSize大于blockSize则可以减少map task的个数。\n\n案例实操：\n\n```shell\nhive (default)> select count(*) from emp;\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n\n# 设置最大切片值为100字节\nhive (default)> set mapreduce.input.fileinputformat.split.maxsize=100;\nhive (default)> select count(*) from emp;\nHadoop job information for Stage-1: number of mappers: 6; number of reducers: 1\n```\n\n合理设置Reduce数：\n\n```shell\n# 方法1\n--（1）参数1：每个Reduce处理的数据量默认是256MB\nhive.exec.reducers.bytes.per.reducer=256000000\n--（2）参数2：每个任务最大的reduce数，默认为1009\nhive.exec.reducers.max=1009\n--（3）计算reducer数的公式\nN=min(参数2，总输入数据量/参数1)\n\n# 方法2\nset mapreduce.job.reduces = 15; \n```\n\n**10.其他**\n\n- 避免使用count(distinct)：单独的distinct执行计划同group by，而count(distinct)会在**一个reducer**里执行去重，优化方式是先在子查询进行GROUP BY或distinct，再求COUNT(*)，但同样需要注意 group by 可能造成的数据倾斜问题。(spark中count(distinct)已经优化，hive未知)\n- 避免笛卡尔积：join的时候不加on条件，或者on条件无效，或显式使用cross join，都会导致笛卡尔积，对笛卡尔积的计算只在**一个reducer**中完成，hive默认禁止笛卡尔积。\n- 小文件开启JVM重用：每一个map或reduce都是一个Java进程，开启JVM需要一定的时空开销，特别是在处理包含大量task(如：大量小文件)的job时，开销是惊人的。`mapreduce.job.jvm.numtasks`可以设定JVM重用的次数，默认为1，设为-1时，表示无限制。JVM重用非常适合处理包含大量小文件的job，因为这些小task的执行时间可能还不及开启JVM的时间，一般可将参数设为10~20，而对于长时间运行的task，JVM重用的提升非常有限，应保证每个task开启新的JVM，因为JVM长时间运行产生的内存碎片反而会降低性能。\n- 推测执行：`set mapred.map.tasks.speculative.execution = true;`默认是 true\n\n",
                            "id": "ef3f7161fd5f",
                            "text": "调优",
                            "layout": null
                        },
                        "children": [
                            {
                                "data": {
                                    "id": "c26117557a22",
                                    "text": "数据倾斜",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "d5e34c2e1c00",
                                    "text": "小文件",
                                    "layout": null
                                },
                                "children": []
                            }
                        ]
                    },
                    {
                        "data": {
                            "id": "8e20595990f0",
                            "text": "典型sql题",
                            "layout": null
                        },
                        "children": []
                    }
                ]
            },
            {
                "data": {
                    "id": "baeb4ad308da",
                    "text": "Kafka",
                    "expandState": "expand",
                    "layout": null
                },
                "children": [
                    {
                        "data": {
                            "id": "177917c9ea53",
                            "text": "计算机专业书籍里有思维导图",
                            "layout": null
                        },
                        "children": []
                    }
                ]
            },
            {
                "data": {
                    "note": "基于内存、快速、可扩展的大数据计算框架，适合迭代计算\n\n快速：主要基于内存，DAG。\n易用：支持Java、python和Scala的API。\n整合：整合了批处理、spark SQL、spark streaming、spark MLlib和图计算(GraphX)。减少了开发和维护的人力成本和部署平台的物力成本。\n兼容：spark可以使用yarn作为它的资源调度器，完美兼容hadoop体系。",
                    "id": "da4701463502",
                    "text": "Spark",
                    "expandState": "collapse",
                    "layout": null
                },
                "children": [
                    {
                        "data": {
                            "note": "Spark Core：核心功能，包含RDD、任务调度、内存管理、错误恢复、与存储系统交互等模块。\nSpark SQL：操作SQL等结构化数据。支持多种数据源，如Hive表、Parquet和JSON。\nSpark Streaming：实时计算。与Spark Core中的 RDD API高度对应。\nSpark MLlib：机器学习。包括分类、回归、聚类、协同过滤等。\nGraghX:图计算\nStandalone调度器：自带的独立调度器",
                            "id": "1b5609abc1b5",
                            "text": "内置模块",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "id": "d33aa2bb6f86",
                            "text": "Spark Core",
                            "layout": null
                        },
                        "children": [
                            {
                                "data": {
                                    "note": "",
                                    "id": "bfd3825bcf3d",
                                    "text": "概念",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "note": "**Master**和**Worker**指节点，在Yarn模式下是ResourceManager和NodeManager\n\n**Driver**和**Executor**指进程：\nDriver是负责执行main()的进程，负责：\n1.将用户程序转化为Application\n2.任务的调度与监控\n3.通过UI展示运行情况\nExecutor是节点上负责运行task的进程，负责：\n1.从线程池抽取空闲线程运行task，并告知driver状态\n2.通过自身的BlockMaanager为task中要缓存的RDD提供内存资源",
                                            "id": "d89150f03a70",
                                            "text": "Master/Worker/Driver/Executor",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "Application:：用户提交的应用程序，可能包含多个job\nJob：一般根据shuffle划分为多个Stage，一个action算子触发一个job\nStage：根据shuffle划分的阶段，一个Stage有一个taskset，包含多个可以并行执行的task\nTask：在Executor进程中执行的具体任务，task数量由分区数决定，每个分区对应一个Task",
                                            "id": "a2e997ef0c39",
                                            "text": "Application/Job/Stage/Task",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            },
                            {
                                "data": {
                                    "note": "1.Client向ResourceManager提交应用\n2.ResourceManager选择一个节点创建ApplicationMaster，并创建Driver进程，准备Driver的运行环境(启动SparkContext、创建DAG Scheduler和Task Scheduler等)，就绪后Driver向ResourceManager注册并申请要启动的Executor资源；\n3.资源就绪时，ResourceManager在合适的节点上启动Executor；\n4.Driver执行main()，通过DAG Scheduler划分stage，从action算子进行反向推算，划分stage的依据是宽依赖，构建好stage后，将各个Stage的Taskset发送给Task Scheduler。\n5.Task Scheduler根据**数据本地性** 和**推测执行** 将Task分配给Executor运行。\n6.Executor通过心跳机制向Driver报告任务执行情况。\n7.运行完毕释放所有资源。\n\n![](https://yeluoqianqiu.github.io/assets/spark_execution.png)",
                                    "id": "6c935b3204a7",
                                    "text": "执行流程",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "eb3a43594a6c",
                                    "text": "stage和task调度",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "f1f19c3dbbdd",
                                    "text": "部署模式",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "在源码中可以看到有对RDD介绍的注释：\n\n- A list of partitions\n- A function for computing each split\n- A list of dependencies on other RDDs \n- Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)\n- Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)",
                                    "id": "b8a1c8d03ed4",
                                    "text": "RDD",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "722591f4d427",
                                    "text": "Spark Shuffle",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "id": "c667c98ea758",
                                            "text": "原理",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "id": "fc5af760fdc6",
                                            "text": "Spark Shuffle与MR Shuffle区别",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            },
                            {
                                "data": {
                                    "id": "1a473018137c",
                                    "text": "累加器与广播变量",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "a0d570781047",
                                    "text": "内存模型",
                                    "layout": null
                                },
                                "children": []
                            }
                        ]
                    },
                    {
                        "data": {
                            "id": "a9bf986224af",
                            "text": "Spark SQL",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "id": "4eef4ec8dcd8",
                            "text": "Spark Streaming",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "id": "9772b6e22198",
                            "text": "Spark执行流程",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "id": "991c1b7c2c1b",
                            "text": "Spark阶段划分DAG",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "id": "8771fcd63d92",
                            "text": "Spark调度算法",
                            "layout": null
                        },
                        "children": []
                    }
                ]
            },
            {
                "data": {
                    "id": "b2af317b9131",
                    "text": "Zookeeper",
                    "layout": null
                },
                "children": []
            },
            {
                "data": {
                    "id": "029125e24b4d",
                    "text": "ClickHouse",
                    "expandState": "collapse",
                    "layout": null
                },
                "children": [
                    {
                        "data": {
                            "note": "分布式数据库，主要用于OLAP场景。\n列式存储+压缩机制，实现每秒几亿行的吞吐量\n分片机制和副本机制：shard和replica，支持多核心，多分片并行查询，\n支持大多数SQL语法，如group by, order by, in, join\n缺点：不支持事务，由于是稀疏索引，不适合通过键值进行单行查询",
                            "id": "a47b22dcda06",
                            "text": "简介",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "note": "1.整型：UInt8,UInt16,UInt32,UInt64,Int8,Int16,Int32,Int64\n范围U开头-2N/2~2N-1;非U开头0～2^N-1\n2.枚举类型：Enum8,Enum16\nEnum('hello'=1,'test'=-1),Enum是有符号的整型映射的，因此负数也是可以的\n3.字符串型：FixedString(N),String\nN是最大字节数，不是字符长度，如果是UTF8字符串，那么就会占3个字节，GBK会占2字节;String可以用来替换VARCHAR,BLOB,CLOB等数据类型\n4.时间类型：Date\n5.数组类型：Array(T)\nT是一个基本类型，包括arry在内，官方不建议使用多维数组\n6.元组：Tuple\n7.结构：Nested(name1 Type1,name2 Type2,...)",
                            "id": "079c6ec56f23",
                            "text": "数据类型",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "id": "e9b85ab3bb44",
                            "text": "物化列",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "id": "de7a2c8b64b8",
                            "text": "引擎",
                            "layout": null
                        },
                        "children": [
                            {
                                "data": {
                                    "note": "适用于小数据量场景，不支持索引、分区，不支持并发读写。",
                                    "id": "a09fdd46a436",
                                    "text": "Log",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "id": "7ae94175f8fb",
                                            "text": "TinyLog",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            },
                            {
                                "data": {
                                    "note": "将数据保存在内存中，数据不会被压缩也不会被格式转换。ClickHouse服务器重启时Memory表内的数据全部丢失。",
                                    "id": "1ab9c5201d27",
                                    "text": "Memory",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "b6f3edee4be9",
                                    "text": "MergeTree系列",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "note": "提供了数据分区、一级索引、二级索引等功能。数据TTL和存储策略",
                                            "id": "ae75395cee9e",
                                            "text": "MergeTree",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "根据主键去重\n",
                                            "id": "81f19c6a68b1",
                                            "text": "ReplacingMergeTre",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "根据预先汇总条件进行汇总",
                                            "id": "8ea9a6769bd5",
                                            "text": "SummingMergeTree",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "按预先定义条件聚合数据，类似数据立方体",
                                            "id": "4dc6fa7872bc",
                                            "text": "AggregatingMergeTree",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "以增代改，通过sign字段标记数据是否有效。",
                                            "id": "466d29789af9",
                                            "text": "CollapsingMergeTree",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            }
                        ]
                    }
                ]
            },
            {
                "data": {
                    "id": "7f855015cbfe",
                    "text": "Linux",
                    "expandState": "collapse",
                    "layout": null
                },
                "children": [
                    {
                        "data": {
                            "note": "基本：ls/ll、cd、mkdir、rm -rf、cp、mv、kill、tar -xvf file.tar\n\n查看进程： ps -ef | grep xxx\nps -aux | grep xxx (-aux显示所有状态)\n\nvi/vim：\ni写入 a追加\n:wq 保存并退出\n:wq! 或shift + z + z强制写入并退出\n:q! 强制退出\ngg 首行 G 末行\nd3d 删除3行\no 下行写入\n\ntop：查看系统各进程的资源占用，看是否有CPU占用过大的进程\n\nless/more\n\ntail/head:\ntail -f *.log 实时查看日志文件底部\n\nnetstat：查看网络状态\n\n强杀进程：\nps -ef | grep xxx 获取到pid\nkill -9 pid",
                            "id": "3ce39c2a7d04",
                            "text": "常用命令",
                            "layout": null
                        },
                        "children": []
                    },
                    {
                        "data": {
                            "id": "e1e14589d76e",
                            "text": "文本工具",
                            "layout": null
                        },
                        "children": [
                            {
                                "data": {
                                    "note": "格式：grep [选项] \"模式\" [文件]\n常用选项：\n| 选项 | 含义                                    |\n| ---- | --------------------------------------- |\n| -E   | 开启扩展正则表达式                      |\n| -i   | 忽略大小写                              |\n| -v   | 反选没有匹配上的                        |\n| -n   | 显示行号                                |\n| -c   | 显示匹配上的行数，-cv表示未匹配上的行数 |\n| -A n | 显示匹配到的字符串所在行及其后n行       |\n| -B n | 显示匹配到的字符串所在行及其前n行       |\n| -C n | 显示匹配到的字符串所在行及其前后各n行   |",
                                    "id": "6b0c3da8e53a",
                                    "text": "grep",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "e3cc98283aa6",
                                    "text": "awk",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "id": "5761fc62d982",
                                    "text": "sed",
                                    "layout": null
                                },
                                "children": []
                            }
                        ]
                    }
                ]
            },
            {
                "data": {
                    "id": "bff7c681288a",
                    "text": "Flink",
                    "layout": null
                },
                "children": []
            },
            {
                "data": {
                    "id": "ck6hqw1sn480",
                    "created": 1653243040750,
                    "text": "离线数仓"
                },
                "children": []
            },
            {
                "data": {
                    "id": "21d27e45800f",
                    "text": "Flink实时数仓",
                    "expandState": "expand",
                    "layout": null
                },
                "children": [
                    {
                        "data": {
                            "id": "cjlvmm176ps0",
                            "created": 1651149441938,
                            "text": "课程介绍",
                            "expandState": "collapse"
                        },
                        "children": [
                            {
                                "data": {
                                    "id": "cjlvlcorb880",
                                    "created": 1651149343231,
                                    "text": "课程重点",
                                    "note": "1. Flink CDC并对比MaxWell、Canal\n2. 动态分流\n3. 多流join或union\n4. 关联维表 定时器、状态编程、Flink SQL、时效性优化：旁路缓存、异步IO"
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "实时计算：考虑时效性，从数据源经过实时计算直接出结果\n实时数仓：对数据分层规划，沉淀中间计算结果，牺牲少量时效性来提高复用性",
                                    "id": "8f762ee0a216",
                                    "text": "实时计算与实时数仓",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": "离线计算：计算之前所有数据已经生成，常常是T+1的任务，一般数据量较大，计算时间较长，对时效性要求低\n实时计算：对源源不断到来的流数据进行计算，一般数据量较小，计算时间短，对时效性要求高",
                                    "id": "5f2e49d6bdca",
                                    "text": "离线计算与实时计算的区别",
                                    "layout": null
                                },
                                "children": []
                            },
                            {
                                "data": {
                                    "note": null,
                                    "id": "f37c16718078",
                                    "text": "实时数仓分层",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "id": "cjlmfzofxw80",
                                            "created": 1651123533104,
                                            "text": "为什么要分层",
                                            "note": "**复用**、解耦、需求拆解",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "id": "cjlmg0v90wg0",
                                            "created": 1651123535692,
                                            "text": "分哪几层",
                                            "note": "- ODS：原始的前端埋点日志数据和后端业务数据(Kafka)\n- DWD：以数据对象为单位进行分流，如：订单、页面访问(Kafka)\n- DIM：维度数据(HBase)\n- DWM：对部分数据对象进行进一步加工，如：独立访问、跳出行为，也可以和维度进行关联，形成宽表，但依然是明细数据。(Kafka)\n- DWS：根据某个主题将多个事实表数据进行轻度聚合，形成主题宽表(ClickHouse)\n- ADS：将Clickhouse中的数据根据**可视化**、**即席查询**等BI应用需求进行筛选聚合",
                                            "layout": null
                                        },
                                        "children": [
                                            {
                                                "data": {
                                                    "id": "cjlml1464ig0",
                                                    "created": 1651123928052,
                                                    "text": "维度表放HBase原因",
                                                    "note": "维度表主要用于与事实表join，为事实表补充维度，维度数据通常在较长时间内不会更新，需要长期保留，而Kafka中的数据则通常只保留几天。",
                                                    "layout": null
                                                },
                                                "children": []
                                            },
                                            {
                                                "data": {
                                                    "id": "cjlmo82uf8g0",
                                                    "created": 1651124178302,
                                                    "text": "即席查询与离线/实时计算的区别",
                                                    "note": "即席查询突出需求的临时性，而离线/实时计算体现需求的固定性和周期性。",
                                                    "layout": null
                                                },
                                                "children": []
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "data": {
                                    "id": "cjlv4quit600",
                                    "created": 1651148041864,
                                    "text": "离线数仓与实时数仓(面试选一种架构进行阐述)",
                                    "expandState": "expand",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "id": "7b21b1b97e84",
                                            "text": "离线数仓架构",
                                            "note": "![image-20220428134303271](https://yeluoqianqiu.github.io/assets/离线数仓架构.png)\n\n优点：耦合性低，稳定性高\n缺点：时效性差一些\n选型理由：\n\n- 大公司追求系统的高稳定性和低耦合性\n- 考虑未来数据量变大，不用换架构\n- 早期使用spark streaming，以微批处理实时业务\n",
                                            "layout": null
                                        },
                                        "children": [
                                            {
                                                "data": {
                                                    "id": "cjlncyc7tkg0",
                                                    "created": 1651126116205,
                                                    "text": "Sqoop",
                                                    "note": "导入数据的方式：\n\n- 全量：`where 1=1`\n- 增量：`where gmt_created='当天'`\n- 新增及变化：`where gmt_created='当天' or gmt_modified='当天'`\n- 特殊：只导入一次",
                                                    "layout": null
                                                },
                                                "children": []
                                            },
                                            {
                                                "data": {
                                                    "id": "cjlnd4blr4w0",
                                                    "created": 1651126129229,
                                                    "text": "Flume",
                                                    "note": "TailDirSource:\n\n- 优点：断点续传，实时监控多目录、多文件\n- 缺点：文件更名后会被重新读取导致数据重复\n- 解决：使用不更名打印日志的框架，或修改源码，让TailDirSource判断文件变更时只看iNode值\n\nKafka Channel:\n\n- 优点：数据直接到达Kafka，不用额外sink；既可作为生产者，又可作为消费者\n- 用法：\n\t- Source-KafkaChannel-Sink\n    - Source-KafkaChannel\n    - KafkaChannel-Sink\n\nFlume消费者的HDFS Sink:\n\n- 如何处理小文件：3种方法",
                                                    "layout": null
                                                },
                                                "children": []
                                            },
                                            {
                                                "data": {
                                                    "id": "cjlnd5qpw0o0",
                                                    "created": 1651126132319,
                                                    "text": "Kafka",
                                                    "note": "**Producer**:\n\n- ACK：0 1 -1 (如何保证生产者不丢数据)\n- 拦截器、序列化器、分区器\n- 发送流程、sender main\n- 幂等性、事务\n- 分区规则：\n    - 指定了分区则发往指定分区\n    - 未指定分区但有key，根据key的hash值发往指定分区\n    - 未指定分区且无key，轮询\n\n**Broker**:\n\n- Topic:\n\t- 副本：高可靠。相关知识点：ISR、LEO、HW\n    - 分区：高并发，负载均衡。\n\n**Consumer**:\n\n- 分区分配规则\n- offset保存：默认`__consumer_offsets`主题，也可手动维护offset，如保存在MySQL中，将保存数据与保存offset在一个事务中完成，实现精准一次消费\n\t- 先保存数据后保存offset：数据重复，若下游能保证幂等性，也能实现精准一次消费(重复数据+幂等性较事务开销低，生产环境常用)\n    - 先保存offset后保存数据：数据丢失",
                                                    "layout": null
                                                },
                                                "children": []
                                            }
                                        ]
                                    },
                                    {
                                        "data": {
                                            "id": "1a6206228812",
                                            "text": "实时数仓架构",
                                            "note": "![image-20220428135950324](https://yeluoqianqiu.github.io/assets/实时数仓架构.png)\n\n优点：时效性好\n缺点：耦合性高，稳定性低\n选型理由：\n\n- 使用Flink，时效性好，实时业务对实时性比较看重\n- Kafka集群高可用，挂一两台也没问题\n- 数据量还不大，所有机器在一个机房，数据传输没有问题",
                                            "layout": null
                                        },
                                        "children": [
                                            {
                                                "data": {
                                                    "id": "cjlvft56oyo0",
                                                    "created": 1651148908868,
                                                    "text": "业务数据",
                                                    "note": "使用Canal/MaxWell/FlinkCDC取代Sqoop(以MR实现)，提高实时性，事项方式都是监控Binlog变化"
                                                },
                                                "children": []
                                            },
                                            {
                                                "data": {
                                                    "id": "cjlvfxk8nxc0",
                                                    "created": 1651148918485,
                                                    "text": "日志数据",
                                                    "note": "不经过Flume，直接将日志数据发往Kafka，减少了磁盘IO，时效性更高，但耦合性变高了。"
                                                },
                                                "children": []
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "data": {
                                    "id": "41f1469c6078",
                                    "text": "常见实时计算需求",
                                    "layout": null
                                },
                                "children": [
                                    {
                                        "data": {
                                            "id": "bd388f3333e6",
                                            "text": "报表或分析图中需要当日部分",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "",
                                            "id": "0cdce8c285e7",
                                            "text": "实时大屏：双十一实时大盘",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "note": "",
                                            "id": "18c3701aefaa",
                                            "text": "数据预警：风控、反欺诈",
                                            "layout": null
                                        },
                                        "children": []
                                    },
                                    {
                                        "data": {
                                            "id": "a229eccacb86",
                                            "text": "实时推荐",
                                            "layout": null
                                        },
                                        "children": []
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "data": {
                            "id": "cjlvmznu42o0",
                            "created": 1651149471605,
                            "text": "数据采集层"
                        },
                        "children": []
                    }
                ]
            }
        ]
    },
    "template": "right",
    "theme": "fresh-blue-compat",
    "version": "1.4.43"
}